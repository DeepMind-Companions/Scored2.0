{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa5617f",
   "metadata": {},
   "source": [
    "# Numpy Network 1\n",
    "\n",
    "---\n",
    "\n",
    "### This is the first neural network I will be implementing\n",
    "\n",
    "- I will be using NumPy to implement this neural network.\n",
    "- This is the first numpy network I will be implementing without any guidance from a course etc.\n",
    "- The initial architecture has not been decided but I will once I implement it.\n",
    "- I am aiming for a NN of 50 - 20 - 10 - 5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216acc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent.parent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8535fca",
   "metadata": {},
   "source": [
    "### We will start by importing and preparing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597a5d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254696, 446)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data = pd.read_pickle(parent_dir/\"Resources\"/\"FinalData\"/\"dev_data.pkl\")\n",
    "test_data = pd.read_pickle(parent_dir/\"Resources\"/\"FinalData\"/\"test_data.pkl\")\n",
    "training_data = pd.read_pickle(parent_dir/\"Resources\"/\"FinalData\"/\"training_data.pkl\")\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b377e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4153: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "training_data_x = training_data.drop(columns = ['Final Output'])\n",
    "training_data_y = training_data['Final Output']\n",
    "training_data_x = training_data_x.reset_index(['Over', 'Ball'])\n",
    "\n",
    "dev_data_x = dev_data.drop(columns = ['Final Output'])\n",
    "dev_data_y = dev_data['Final Output']\n",
    "dev_data_x = dev_data_x.reset_index(['Over', 'Ball'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1cc276e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Over', 'Ball', 'Player Status', 'Inning Data', 'Batting Stats',\n",
       "       'Bowling Stats', 'Ground Data'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_x.columns.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "332c08b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Inning Data</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>Total Wickets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">352674</th>\n",
       "      <th>England</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>England</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>England</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>England</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>England</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1361770</th>\n",
       "      <th>Malaysia</th>\n",
       "      <td>151.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <td>152.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <td>156.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <td>157.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <td>163.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23552 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Inning Data              \n",
       "                       score Total Wickets\n",
       "        Country                           \n",
       "352674  England          4.0           0.0\n",
       "        England          4.0           0.0\n",
       "        England          5.0           0.0\n",
       "        England          5.0           0.0\n",
       "        England          9.0           0.0\n",
       "...                      ...           ...\n",
       "1361770 Malaysia       151.0           7.0\n",
       "        Malaysia       152.0           7.0\n",
       "        Malaysia       156.0           7.0\n",
       "        Malaysia       157.0           7.0\n",
       "        Malaysia       163.0           7.0\n",
       "\n",
       "[23552 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_x['Inning Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f31f3c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254696, 447)\n",
      "(254696, 1)\n",
      "(23552, 447)\n",
      "(23552, 1)\n"
     ]
    }
   ],
   "source": [
    "print(training_data_x.shape)\n",
    "print(training_data_y.shape)\n",
    "print(dev_data_x.shape)\n",
    "print(dev_data_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4607b",
   "metadata": {},
   "source": [
    "NumPy Implementation of final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec2944e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447, 254696)\n",
      "(1, 254696)\n",
      "(447, 23552)\n",
      "(1, 23552)\n"
     ]
    }
   ],
   "source": [
    "train_x = training_data_x.to_numpy().T\n",
    "train_y = training_data_y.to_numpy().T\n",
    "dev_x = dev_data_x.to_numpy().T\n",
    "dev_y = dev_data_y.to_numpy().T\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c877e57",
   "metadata": {},
   "source": [
    "Using a Mask to get only the final 10 overs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1718116",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_overs = 0\n",
    "\n",
    "train_mask = (training_data_x['Over'] > train_overs).to_numpy()\n",
    "dev_mask = (dev_data_x['Over'] > train_overs).to_numpy()\n",
    "\n",
    "train_x = train_x[:, train_mask]\n",
    "train_y = train_y[:, train_mask]\n",
    "dev_x = dev_x[:, dev_mask]\n",
    "dev_y = dev_y[:, dev_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de421049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 240679)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "346de1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "diff = train_x.shape[1] % 256\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cad14a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447, 240896)\n",
      "(1, 240896)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_x_batched = np.hstack((train_x, train_x[:,0:256 - diff]))\n",
    "train_y_batched = np.hstack((train_y, train_y[:,0:256 - diff]))\n",
    "print(train_x_batched.shape)\n",
    "print(train_y_batched.shape)\n",
    "print(train_x_batched.shape[1] % 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c90577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_batched[:, 32:64].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f6c4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomising data\n",
    "random_mask = np.random.permutation(train_y_batched.shape[1])\n",
    "train_x_batched = train_x_batched[:, random_mask]\n",
    "train_y_batched = train_y_batched[:, random_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1db67",
   "metadata": {},
   "source": [
    "### Time to define a few important variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d731d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n",
      "941\n"
     ]
    }
   ],
   "source": [
    "input_size = train_x_batched.shape[0]\n",
    "print(input_size)\n",
    "hidden_layers = 5\n",
    "output_size = 1\n",
    "num_batches = train_y_batched.shape[1] // 256\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a03c7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(num):\n",
    "    return num * (num > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "757f0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptim:\n",
    "    def __init__(self, eta=0.0001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.m_dw, self.v_dw = 0, 0\n",
    "        self.m_db, self.v_db = 0, 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "    def update(self, t, w, b, dw, db):\n",
    "        ## dw, db are from current minibatch\n",
    "        ## momentum beta 1\n",
    "        # *** weights *** #\n",
    "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
    "        # *** biases *** #\n",
    "        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n",
    "\n",
    "        ## rms beta 2\n",
    "        # *** weights *** #\n",
    "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
    "        # *** biases *** #\n",
    "        self.v_db = self.beta2*self.v_db + (1-self.beta2)*(db**2)\n",
    "\n",
    "        ## bias correction\n",
    "        m_dw_corr = self.m_dw/(1-self.beta1**t)\n",
    "        m_db_corr = self.m_db/(1-self.beta1**t)\n",
    "        v_dw_corr = self.v_dw/(1-self.beta2**t)\n",
    "        v_db_corr = self.v_db/(1-self.beta2**t)\n",
    "\n",
    "        ## update weights and biases\n",
    "        w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
    "        b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34db9b",
   "metadata": {},
   "source": [
    "### Let's define a class for a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15fb3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, input_size, units):\n",
    "        self.units = units\n",
    "        self.weights = np.random.randn(units, input_size)*0.0001\n",
    "        self.bias = np.zeros((units, 1))\n",
    "        self.optimizer = AdamOptim()\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.x = inp\n",
    "        self.m = inp.shape[1]\n",
    "        self.y = np.dot(self.weights, inp) + self.bias\n",
    "        return self.y\n",
    "    \n",
    "    def back_prop(self, dy):\n",
    "        self.dW = (1/self.m) * np.dot(dy, self.x.T)\n",
    "        self.db = (1/self.m) * np.sum(dy, axis = 1, keepdims = True)\n",
    "        self.dx = np.dot(self.weights.T, dy)\n",
    "        return self.dx\n",
    "    \n",
    "    \n",
    "    \n",
    "    def grad_des(self, t):\n",
    "        self.weights, self.bias = self.optimizer.update(t, self.weights, self.bias, self.dW, self.db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15be9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hidden_layer (layer):\n",
    "    \n",
    "    def __init__(self, input_size, units):\n",
    "        super().__init__(input_size, units)\n",
    "    \n",
    "    def __call__(self, inp):\n",
    "        super().__call__(inp)\n",
    "        self.z = relu(self.y)\n",
    "        return self.z\n",
    "        \n",
    "    def back_prop(self, dz):\n",
    "        dy = dz * (self.y > 0)\n",
    "        self.dy = dy\n",
    "        super().back_prop(dy)\n",
    "        return self.dx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "568aadcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "''' class final_layer:\n",
    "    def __init__(self, input_size, units):\n",
    "        self.units = units\n",
    "        self.weights = np.random.randn(units, input_size)\n",
    "        self.bias = np.zeros((units, 1))\n",
    "        self.m = self.inp.shape[1]\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.x = inp\n",
    "        self.z = np.dot(self.weights, inp) + self.bias\n",
    "    \n",
    "    def back_prop(self, dy, learning_rate, beta1, beta2):\n",
    "        self.dW = (1/self.m) * np.dot(dy, self.x.T)\n",
    "        self.db = (1/self.m) * np.sum(dy, axis = 1, keepdims = True)\n",
    "        self.dx = np.dot(W.T, dy)\n",
    "        return self.dx\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "628735df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequence:\n",
    "    def __init__(self, layer_list):\n",
    "        self.layers = layer_list\n",
    "    \n",
    "    def forward_propagate(self, inp):\n",
    "        y = inp\n",
    "        for layer in self.layers:\n",
    "            y = layer(y)\n",
    "        return y\n",
    "    \n",
    "    def calculate_loss(self, y_pred, y_true):\n",
    "        mse = np.mean((y_true - y_pred)**2)\n",
    "        self.loss = mse\n",
    "        return mse\n",
    "    \n",
    "    def back_propagate(self, y_pred, y_true):\n",
    "        dz = 2*(y_pred - y_true)\n",
    "        for layer in self.layers[::-1]:\n",
    "            dz = layer.back_prop(dz)\n",
    "    \n",
    "    def grad_descent(self, t):\n",
    "        for layer in self.layers:\n",
    "            layer.grad_des(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5363861",
   "metadata": {},
   "source": [
    "### Defining a Demo model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "77cf5822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15., 20., 25., 30., 35.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_demo = sequence([\n",
    "    hidden_layer(1, 3),\n",
    "    layer(3, 1)\n",
    "])\n",
    "\n",
    "demo_data_x = np.array([[1, 2, 3, 4, 5]])\n",
    "demo_data_y = (demo_data_x * 5) + 10\n",
    "demo_data_x.shape\n",
    "\n",
    "y_pred = model_demo.forward_propagate(demo_data_x)\n",
    "\n",
    "model_demo.calculate_loss(y_pred, demo_data_y)\n",
    "\n",
    "losses = []\n",
    "for x in range(1, 100000):\n",
    "    y_pred = model_demo.forward_propagate(demo_data_x)\n",
    "    loss = model_demo.calculate_loss(y_pred, demo_data_y)\n",
    "    losses.append(loss)\n",
    "    model_demo.back_propagate(y_pred, demo_data_y)\n",
    "    model_demo.grad_descent(x)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bf7b1c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[30:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3572a5ef",
   "metadata": {},
   "source": [
    "## Training the actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "071c746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "941\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 200\n",
    "num_batches = train_y_batched.shape[1] // 256\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "119df4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = sequence([\n",
    "    hidden_layer(input_size, 100),\n",
    "    hidden_layer(100, 50),\n",
    "    hidden_layer(50, 50),\n",
    "    hidden_layer(50, 30),\n",
    "    hidden_layer(30, 20),\n",
    "    hidden_layer(20, 10),\n",
    "    hidden_layer(10, 7),\n",
    "    hidden_layer(7, 5),\n",
    "    layer(5, 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "323e34ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ------------- Batch 0:\n",
      "Training Set Loss: 0 -------- Dev Set Loss: 23672.81241508152\n",
      "Epoch 0 ------------- Batch 200:\n",
      "Training Set Loss: 24209.61728449424 -------- Dev Set Loss: 23664.683309529304\n",
      "Epoch 0 ------------- Batch 400:\n",
      "Training Set Loss: 3069.341127461382 -------- Dev Set Loss: 3222.4534532088223\n",
      "Epoch 0 ------------- Batch 600:\n",
      "Training Set Loss: 2931.211090813277 -------- Dev Set Loss: 2892.3677414931217\n",
      "Epoch 0 ------------- Batch 800:\n",
      "Training Set Loss: 2478.0313442329243 -------- Dev Set Loss: 2550.3640089133737\n",
      "Epoch 1 ------------- Batch 0:\n",
      "Training Set Loss: 2508.9540335872143 -------- Dev Set Loss: 2290.0162510590976\n",
      "Epoch 1 ------------- Batch 200:\n",
      "Training Set Loss: 1986.4130457909814 -------- Dev Set Loss: 1888.672462877989\n",
      "Epoch 1 ------------- Batch 400:\n",
      "Training Set Loss: 1585.9373999420484 -------- Dev Set Loss: 1514.414015477372\n",
      "Epoch 1 ------------- Batch 600:\n",
      "Training Set Loss: 1440.823819478143 -------- Dev Set Loss: 1251.2705544529763\n",
      "Epoch 1 ------------- Batch 800:\n",
      "Training Set Loss: 1299.6201102613636 -------- Dev Set Loss: 1071.4173243684386\n",
      "Epoch 2 ------------- Batch 0:\n",
      "Training Set Loss: 1199.070504903831 -------- Dev Set Loss: 983.1660435005113\n",
      "Epoch 2 ------------- Batch 200:\n",
      "Training Set Loss: 1168.8941858404712 -------- Dev Set Loss: 907.9375438488601\n",
      "Epoch 2 ------------- Batch 400:\n",
      "Training Set Loss: 1027.0010905351485 -------- Dev Set Loss: 868.3278640869297\n",
      "Epoch 2 ------------- Batch 600:\n",
      "Training Set Loss: 1095.5903292029695 -------- Dev Set Loss: 836.5208139088958\n",
      "Epoch 2 ------------- Batch 800:\n",
      "Training Set Loss: 957.2421670332186 -------- Dev Set Loss: 813.9384642685125\n",
      "Epoch 3 ------------- Batch 0:\n",
      "Training Set Loss: 978.2733960665895 -------- Dev Set Loss: 825.8986476938882\n",
      "Epoch 3 ------------- Batch 200:\n",
      "Training Set Loss: 1034.824275705279 -------- Dev Set Loss: 816.1340360564889\n",
      "Epoch 3 ------------- Batch 400:\n",
      "Training Set Loss: 955.6741970642233 -------- Dev Set Loss: 812.7555128454333\n",
      "Epoch 3 ------------- Batch 600:\n",
      "Training Set Loss: 1041.4554954066134 -------- Dev Set Loss: 792.3263243613886\n",
      "Epoch 3 ------------- Batch 800:\n",
      "Training Set Loss: 901.6433574088608 -------- Dev Set Loss: 777.5798869646593\n",
      "Epoch 4 ------------- Batch 0:\n",
      "Training Set Loss: 922.3276865631038 -------- Dev Set Loss: 796.5812335577358\n",
      "Epoch 4 ------------- Batch 200:\n",
      "Training Set Loss: 989.0572091322572 -------- Dev Set Loss: 786.9350707580008\n",
      "Epoch 4 ------------- Batch 400:\n",
      "Training Set Loss: 925.5937433451919 -------- Dev Set Loss: 785.2801490177391\n",
      "Epoch 4 ------------- Batch 600:\n",
      "Training Set Loss: 997.8171442480185 -------- Dev Set Loss: 763.7992954806075\n",
      "Epoch 4 ------------- Batch 800:\n",
      "Training Set Loss: 870.0680234017889 -------- Dev Set Loss: 749.725718489861\n",
      "Epoch 5 ------------- Batch 0:\n",
      "Training Set Loss: 877.2299455750023 -------- Dev Set Loss: 767.7379171517479\n",
      "Epoch 5 ------------- Batch 200:\n",
      "Training Set Loss: 951.4126790575842 -------- Dev Set Loss: 759.8760984916713\n",
      "Epoch 5 ------------- Batch 400:\n",
      "Training Set Loss: 897.7739229410752 -------- Dev Set Loss: 758.1631828252907\n",
      "Epoch 5 ------------- Batch 600:\n",
      "Training Set Loss: 956.5524198682269 -------- Dev Set Loss: 739.6965814667565\n",
      "Epoch 5 ------------- Batch 800:\n",
      "Training Set Loss: 840.7205863468777 -------- Dev Set Loss: 726.0866518288001\n",
      "Epoch 6 ------------- Batch 0:\n",
      "Training Set Loss: 836.2285926658018 -------- Dev Set Loss: 742.9588010835287\n",
      "Epoch 6 ------------- Batch 200:\n",
      "Training Set Loss: 917.7286527254682 -------- Dev Set Loss: 736.5802051130017\n",
      "Epoch 6 ------------- Batch 400:\n",
      "Training Set Loss: 870.6182995538138 -------- Dev Set Loss: 733.8122231167021\n",
      "Epoch 6 ------------- Batch 600:\n",
      "Training Set Loss: 918.8950692070644 -------- Dev Set Loss: 720.0789771409538\n",
      "Epoch 6 ------------- Batch 800:\n",
      "Training Set Loss: 812.7616020205171 -------- Dev Set Loss: 706.4757130767092\n",
      "Epoch 7 ------------- Batch 0:\n",
      "Training Set Loss: 798.9428476063739 -------- Dev Set Loss: 722.1725059444718\n",
      "Epoch 7 ------------- Batch 200:\n",
      "Training Set Loss: 887.145240681351 -------- Dev Set Loss: 716.3432442628789\n",
      "Epoch 7 ------------- Batch 400:\n",
      "Training Set Loss: 842.6827048477742 -------- Dev Set Loss: 712.1461049082293\n",
      "Epoch 7 ------------- Batch 600:\n",
      "Training Set Loss: 884.3799049329516 -------- Dev Set Loss: 703.4677664475906\n",
      "Epoch 7 ------------- Batch 800:\n",
      "Training Set Loss: 785.6666501330251 -------- Dev Set Loss: 689.4057920204588\n",
      "Epoch 8 ------------- Batch 0:\n",
      "Training Set Loss: 763.9963783879654 -------- Dev Set Loss: 703.8725965145879\n",
      "Epoch 8 ------------- Batch 200:\n",
      "Training Set Loss: 857.7881419164264 -------- Dev Set Loss: 697.3559139752626\n",
      "Epoch 8 ------------- Batch 400:\n",
      "Training Set Loss: 812.2808202806973 -------- Dev Set Loss: 691.9362310935237\n",
      "Epoch 8 ------------- Batch 600:\n",
      "Training Set Loss: 851.7994674481273 -------- Dev Set Loss: 688.0676337966632\n",
      "Epoch 8 ------------- Batch 800:\n",
      "Training Set Loss: 759.1048111786791 -------- Dev Set Loss: 673.3146642945674\n",
      "Epoch 9 ------------- Batch 0:\n",
      "Training Set Loss: 729.9342864494481 -------- Dev Set Loss: 686.7114929526393\n",
      "Epoch 9 ------------- Batch 200:\n",
      "Training Set Loss: 827.5837292713763 -------- Dev Set Loss: 678.2075279351459\n",
      "Epoch 9 ------------- Batch 400:\n",
      "Training Set Loss: 778.675970839696 -------- Dev Set Loss: 672.3787256065306\n",
      "Epoch 9 ------------- Batch 600:\n",
      "Training Set Loss: 819.567217569759 -------- Dev Set Loss: 672.3842182623148\n",
      "Epoch 9 ------------- Batch 800:\n",
      "Training Set Loss: 733.5060868830069 -------- Dev Set Loss: 657.4787427742034\n",
      "Epoch 10 ------------- Batch 0:\n",
      "Training Set Loss: 696.3633969905281 -------- Dev Set Loss: 670.1037347720973\n",
      "Epoch 10 ------------- Batch 200:\n",
      "Training Set Loss: 795.7287365288039 -------- Dev Set Loss: 658.9859968658747\n",
      "Epoch 10 ------------- Batch 400:\n",
      "Training Set Loss: 742.4383041262552 -------- Dev Set Loss: 653.5797926044315\n",
      "Epoch 10 ------------- Batch 600:\n",
      "Training Set Loss: 785.8580474076407 -------- Dev Set Loss: 655.3670010171392\n",
      "Epoch 10 ------------- Batch 800:\n",
      "Training Set Loss: 710.0658224304758 -------- Dev Set Loss: 642.3439330580837\n",
      "Epoch 11 ------------- Batch 0:\n",
      "Training Set Loss: 663.6846664281121 -------- Dev Set Loss: 653.728485299578\n",
      "Epoch 11 ------------- Batch 200:\n",
      "Training Set Loss: 764.0953317749693 -------- Dev Set Loss: 641.6928414047684\n",
      "Epoch 11 ------------- Batch 400:\n",
      "Training Set Loss: 705.5716453958197 -------- Dev Set Loss: 636.9893413190547\n",
      "Epoch 11 ------------- Batch 600:\n",
      "Training Set Loss: 750.6100380999396 -------- Dev Set Loss: 638.1746899239632\n",
      "Epoch 11 ------------- Batch 800:\n",
      "Training Set Loss: 690.3020203207817 -------- Dev Set Loss: 629.4642541989064\n",
      "Epoch 12 ------------- Batch 0:\n",
      "Training Set Loss: 632.6170468559471 -------- Dev Set Loss: 637.9391367354688\n",
      "Epoch 12 ------------- Batch 200:\n",
      "Training Set Loss: 736.3066899593547 -------- Dev Set Loss: 628.6166030323478\n",
      "Epoch 12 ------------- Batch 400:\n",
      "Training Set Loss: 672.2068479400051 -------- Dev Set Loss: 625.1229833254163\n",
      "Epoch 12 ------------- Batch 600:\n",
      "Training Set Loss: 718.8154044511939 -------- Dev Set Loss: 625.0246277061206\n",
      "Epoch 12 ------------- Batch 800:\n",
      "Training Set Loss: 675.4873419977953 -------- Dev Set Loss: 620.6143418137789\n",
      "Epoch 13 ------------- Batch 0:\n",
      "Training Set Loss: 604.539506507852 -------- Dev Set Loss: 625.1438460262597\n",
      "Epoch 13 ------------- Batch 200:\n",
      "Training Set Loss: 715.4627319592586 -------- Dev Set Loss: 620.7887602255767\n",
      "Epoch 13 ------------- Batch 400:\n",
      "Training Set Loss: 645.788630151635 -------- Dev Set Loss: 618.6663468498554\n",
      "Epoch 13 ------------- Batch 600:\n",
      "Training Set Loss: 695.1547262162999 -------- Dev Set Loss: 617.5260867640952\n",
      "Epoch 13 ------------- Batch 800:\n",
      "Training Set Loss: 665.855453241506 -------- Dev Set Loss: 615.9329486121935\n",
      "Epoch 14 ------------- Batch 0:\n",
      "Training Set Loss: 581.7341188182365 -------- Dev Set Loss: 617.317267217753\n",
      "Epoch 14 ------------- Batch 200:\n",
      "Training Set Loss: 701.7468188016 -------- Dev Set Loss: 617.1821044087565\n",
      "Epoch 14 ------------- Batch 400:\n",
      "Training Set Loss: 626.8193485003736 -------- Dev Set Loss: 615.9710505669527\n",
      "Epoch 14 ------------- Batch 600:\n",
      "Training Set Loss: 679.5256725486981 -------- Dev Set Loss: 614.1759802905832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 ------------- Batch 800:\n",
      "Training Set Loss: 659.3713689621648 -------- Dev Set Loss: 614.3790713612201\n",
      "Epoch 15 ------------- Batch 0:\n",
      "Training Set Loss: 565.1889364873699 -------- Dev Set Loss: 613.7511943452788\n",
      "Epoch 15 ------------- Batch 200:\n",
      "Training Set Loss: 692.6132879679105 -------- Dev Set Loss: 616.0160465689517\n",
      "Epoch 15 ------------- Batch 400:\n",
      "Training Set Loss: 613.8081574305788 -------- Dev Set Loss: 615.3138303639093\n",
      "Epoch 15 ------------- Batch 600:\n",
      "Training Set Loss: 669.4954506842238 -------- Dev Set Loss: 612.860563652592\n",
      "Epoch 15 ------------- Batch 800:\n",
      "Training Set Loss: 654.6621280247583 -------- Dev Set Loss: 614.0536594974052\n",
      "Epoch 16 ------------- Batch 0:\n",
      "Training Set Loss: 553.0388517230848 -------- Dev Set Loss: 612.0317980001286\n",
      "Epoch 16 ------------- Batch 200:\n",
      "Training Set Loss: 686.5929357393593 -------- Dev Set Loss: 615.4085494167346\n",
      "Epoch 16 ------------- Batch 400:\n",
      "Training Set Loss: 604.6213158865685 -------- Dev Set Loss: 615.1559434250672\n",
      "Epoch 16 ------------- Batch 600:\n",
      "Training Set Loss: 662.624029783662 -------- Dev Set Loss: 611.9524983157247\n",
      "Epoch 16 ------------- Batch 800:\n",
      "Training Set Loss: 651.167853328345 -------- Dev Set Loss: 613.5416807120504\n",
      "Epoch 17 ------------- Batch 0:\n",
      "Training Set Loss: 543.6594495166715 -------- Dev Set Loss: 610.7174273834073\n",
      "Epoch 17 ------------- Batch 200:\n",
      "Training Set Loss: 681.8675180763013 -------- Dev Set Loss: 614.8290486672871\n",
      "Epoch 17 ------------- Batch 400:\n",
      "Training Set Loss: 597.8769254419688 -------- Dev Set Loss: 614.8640821460071\n",
      "Epoch 17 ------------- Batch 600:\n",
      "Training Set Loss: 656.9458557051813 -------- Dev Set Loss: 611.1072391720882\n",
      "Epoch 17 ------------- Batch 800:\n",
      "Training Set Loss: 647.8620863519134 -------- Dev Set Loss: 613.1207344069339\n",
      "Epoch 18 ------------- Batch 0:\n",
      "Training Set Loss: 536.0065319558773 -------- Dev Set Loss: 609.6107754451143\n",
      "Epoch 18 ------------- Batch 200:\n",
      "Training Set Loss: 677.5513607679283 -------- Dev Set Loss: 614.0524477781802\n",
      "Epoch 18 ------------- Batch 400:\n",
      "Training Set Loss: 592.5351395762584 -------- Dev Set Loss: 614.2991733647779\n",
      "Epoch 18 ------------- Batch 600:\n",
      "Training Set Loss: 651.934925042783 -------- Dev Set Loss: 610.1434065086185\n",
      "Epoch 18 ------------- Batch 800:\n",
      "Training Set Loss: 644.9378432490632 -------- Dev Set Loss: 612.4189859369335\n",
      "Epoch 19 ------------- Batch 0:\n",
      "Training Set Loss: 529.381111964211 -------- Dev Set Loss: 608.2860703794503\n",
      "Epoch 19 ------------- Batch 200:\n",
      "Training Set Loss: 673.2109181389485 -------- Dev Set Loss: 612.7664671177698\n",
      "Epoch 19 ------------- Batch 400:\n",
      "Training Set Loss: 588.1656162149432 -------- Dev Set Loss: 613.161482683171\n",
      "Epoch 19 ------------- Batch 600:\n",
      "Training Set Loss: 647.0932290118568 -------- Dev Set Loss: 608.7512114076036\n",
      "Epoch 19 ------------- Batch 800:\n",
      "Training Set Loss: 642.1944582998348 -------- Dev Set Loss: 611.167827479224\n",
      "Epoch 20 ------------- Batch 0:\n",
      "Training Set Loss: 523.7555639672107 -------- Dev Set Loss: 606.7862550867573\n",
      "Epoch 20 ------------- Batch 200:\n",
      "Training Set Loss: 668.9060584405895 -------- Dev Set Loss: 611.0955711467694\n",
      "Epoch 20 ------------- Batch 400:\n",
      "Training Set Loss: 584.5092255539221 -------- Dev Set Loss: 611.9057803149332\n",
      "Epoch 20 ------------- Batch 600:\n",
      "Training Set Loss: 642.5091043566679 -------- Dev Set Loss: 607.1342681508038\n",
      "Epoch 20 ------------- Batch 800:\n",
      "Training Set Loss: 639.2367361150909 -------- Dev Set Loss: 609.7649354712415\n",
      "Epoch 21 ------------- Batch 0:\n",
      "Training Set Loss: 518.9052094687469 -------- Dev Set Loss: 605.0686776335077\n",
      "Epoch 21 ------------- Batch 200:\n",
      "Training Set Loss: 664.9713914921118 -------- Dev Set Loss: 609.2272582413289\n",
      "Epoch 21 ------------- Batch 400:\n",
      "Training Set Loss: 581.469081896116 -------- Dev Set Loss: 610.2670362765389\n",
      "Epoch 21 ------------- Batch 600:\n",
      "Training Set Loss: 637.9888505798219 -------- Dev Set Loss: 605.360336479119\n",
      "Epoch 21 ------------- Batch 800:\n",
      "Training Set Loss: 636.0055402170336 -------- Dev Set Loss: 608.1732348233621\n",
      "Epoch 22 ------------- Batch 0:\n",
      "Training Set Loss: 514.872645199808 -------- Dev Set Loss: 602.9849304844973\n",
      "Epoch 22 ------------- Batch 200:\n",
      "Training Set Loss: 660.8680148584119 -------- Dev Set Loss: 606.8865305891903\n",
      "Epoch 22 ------------- Batch 400:\n",
      "Training Set Loss: 578.3768522619444 -------- Dev Set Loss: 608.4530735708555\n",
      "Epoch 22 ------------- Batch 600:\n",
      "Training Set Loss: 633.1258044864426 -------- Dev Set Loss: 603.4417934160281\n",
      "Epoch 22 ------------- Batch 800:\n",
      "Training Set Loss: 632.3822684219269 -------- Dev Set Loss: 606.6769922538714\n",
      "Epoch 23 ------------- Batch 0:\n",
      "Training Set Loss: 511.5305832998549 -------- Dev Set Loss: 601.1822874537974\n",
      "Epoch 23 ------------- Batch 200:\n",
      "Training Set Loss: 657.0284638372494 -------- Dev Set Loss: 605.0660462236609\n",
      "Epoch 23 ------------- Batch 400:\n",
      "Training Set Loss: 575.5136614065101 -------- Dev Set Loss: 607.1470150071866\n",
      "Epoch 23 ------------- Batch 600:\n",
      "Training Set Loss: 628.6634999431135 -------- Dev Set Loss: 601.9974781785122\n",
      "Epoch 23 ------------- Batch 800:\n",
      "Training Set Loss: 629.3218437043802 -------- Dev Set Loss: 605.5297474920706\n",
      "Epoch 24 ------------- Batch 0:\n",
      "Training Set Loss: 508.4515916207804 -------- Dev Set Loss: 599.7689904584192\n",
      "Epoch 24 ------------- Batch 200:\n",
      "Training Set Loss: 653.0142063786371 -------- Dev Set Loss: 603.5621451235969\n",
      "Epoch 24 ------------- Batch 400:\n",
      "Training Set Loss: 573.0324563343968 -------- Dev Set Loss: 606.0177613562865\n",
      "Epoch 24 ------------- Batch 600:\n",
      "Training Set Loss: 624.2880250701653 -------- Dev Set Loss: 600.7845872121154\n",
      "Epoch 24 ------------- Batch 800:\n",
      "Training Set Loss: 626.4784809723601 -------- Dev Set Loss: 604.6819317600855\n",
      "Epoch 25 ------------- Batch 0:\n",
      "Training Set Loss: 505.33935108190155 -------- Dev Set Loss: 598.5933929679121\n",
      "Epoch 25 ------------- Batch 200:\n",
      "Training Set Loss: 648.930409717348 -------- Dev Set Loss: 602.3696800755387\n",
      "Epoch 25 ------------- Batch 400:\n",
      "Training Set Loss: 570.3444508752111 -------- Dev Set Loss: 605.1647596817537\n",
      "Epoch 25 ------------- Batch 600:\n",
      "Training Set Loss: 619.8028280694155 -------- Dev Set Loss: 599.6639416423135\n",
      "Epoch 25 ------------- Batch 800:\n",
      "Training Set Loss: 623.615201750629 -------- Dev Set Loss: 603.7899407937714\n",
      "Epoch 26 ------------- Batch 0:\n",
      "Training Set Loss: 501.80140733956017 -------- Dev Set Loss: 597.4371835756643\n",
      "Epoch 26 ------------- Batch 200:\n",
      "Training Set Loss: 645.0507723072437 -------- Dev Set Loss: 601.0302578573505\n",
      "Epoch 26 ------------- Batch 400:\n",
      "Training Set Loss: 566.9937447116092 -------- Dev Set Loss: 603.7895581604465\n",
      "Epoch 26 ------------- Batch 600:\n",
      "Training Set Loss: 614.6640917120595 -------- Dev Set Loss: 598.1417239728262\n",
      "Epoch 26 ------------- Batch 800:\n",
      "Training Set Loss: 621.0218149995135 -------- Dev Set Loss: 602.8066065970739\n",
      "Epoch 27 ------------- Batch 0:\n",
      "Training Set Loss: 498.79952089742625 -------- Dev Set Loss: 596.2659088383853\n",
      "Epoch 27 ------------- Batch 200:\n",
      "Training Set Loss: 641.2136549343761 -------- Dev Set Loss: 599.9409598942832\n",
      "Epoch 27 ------------- Batch 400:\n",
      "Training Set Loss: 564.1892076792599 -------- Dev Set Loss: 602.8788470151242\n",
      "Epoch 27 ------------- Batch 600:\n",
      "Training Set Loss: 609.2999295082885 -------- Dev Set Loss: 597.1025087224028\n",
      "Epoch 27 ------------- Batch 800:\n",
      "Training Set Loss: 618.8931939858883 -------- Dev Set Loss: 602.1619049619022\n",
      "Epoch 28 ------------- Batch 0:\n",
      "Training Set Loss: 495.7889961644811 -------- Dev Set Loss: 595.2436688295538\n",
      "Epoch 28 ------------- Batch 200:\n",
      "Training Set Loss: 637.3852142348792 -------- Dev Set Loss: 598.9109159334638\n",
      "Epoch 28 ------------- Batch 400:\n",
      "Training Set Loss: 561.2366124469321 -------- Dev Set Loss: 601.8323893461546\n",
      "Epoch 28 ------------- Batch 600:\n",
      "Training Set Loss: 602.9255121493702 -------- Dev Set Loss: 595.7053497525885\n",
      "Epoch 28 ------------- Batch 800:\n",
      "Training Set Loss: 616.8730602572437 -------- Dev Set Loss: 601.2253820719412\n",
      "Epoch 29 ------------- Batch 0:\n",
      "Training Set Loss: 492.5920538118522 -------- Dev Set Loss: 593.816411595806\n",
      "Epoch 29 ------------- Batch 200:\n",
      "Training Set Loss: 633.7355439818366 -------- Dev Set Loss: 597.7357388549051\n",
      "Epoch 29 ------------- Batch 400:\n",
      "Training Set Loss: 557.7871673380338 -------- Dev Set Loss: 600.7147690710904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 ------------- Batch 600:\n",
      "Training Set Loss: 596.0297179514882 -------- Dev Set Loss: 594.6485802033162\n",
      "Epoch 29 ------------- Batch 800:\n",
      "Training Set Loss: 614.6463712438679 -------- Dev Set Loss: 600.559335451608\n",
      "Epoch 30 ------------- Batch 0:\n",
      "Training Set Loss: 489.36399836918145 -------- Dev Set Loss: 593.0239096360864\n",
      "Epoch 30 ------------- Batch 200:\n",
      "Training Set Loss: 630.1522614134117 -------- Dev Set Loss: 597.1360827164887\n",
      "Epoch 30 ------------- Batch 400:\n",
      "Training Set Loss: 554.3153373036466 -------- Dev Set Loss: 600.2523172306048\n",
      "Epoch 30 ------------- Batch 600:\n",
      "Training Set Loss: 588.3738339366057 -------- Dev Set Loss: 593.9791344234117\n",
      "Epoch 30 ------------- Batch 800:\n",
      "Training Set Loss: 611.5032217558114 -------- Dev Set Loss: 600.2784676980203\n",
      "Epoch 31 ------------- Batch 0:\n",
      "Training Set Loss: 485.9197143395992 -------- Dev Set Loss: 592.2387947841902\n",
      "Epoch 31 ------------- Batch 200:\n",
      "Training Set Loss: 626.4202653867513 -------- Dev Set Loss: 596.5996864517335\n",
      "Epoch 31 ------------- Batch 400:\n",
      "Training Set Loss: 550.4651583999824 -------- Dev Set Loss: 599.9905221571436\n",
      "Epoch 31 ------------- Batch 600:\n",
      "Training Set Loss: 580.8609657541037 -------- Dev Set Loss: 592.9627557758224\n",
      "Epoch 31 ------------- Batch 800:\n",
      "Training Set Loss: 609.1015369732029 -------- Dev Set Loss: 600.4609919499563\n",
      "Epoch 32 ------------- Batch 0:\n",
      "Training Set Loss: 482.4083314827673 -------- Dev Set Loss: 591.8748534651533\n",
      "Epoch 32 ------------- Batch 200:\n",
      "Training Set Loss: 623.4499888330079 -------- Dev Set Loss: 596.6610209432899\n",
      "Epoch 32 ------------- Batch 400:\n",
      "Training Set Loss: 547.032569493195 -------- Dev Set Loss: 599.7510314191707\n",
      "Epoch 32 ------------- Batch 600:\n",
      "Training Set Loss: 574.062995480174 -------- Dev Set Loss: 592.4729281701616\n",
      "Epoch 32 ------------- Batch 800:\n",
      "Training Set Loss: 606.630200195971 -------- Dev Set Loss: 600.8996945285948\n",
      "Epoch 33 ------------- Batch 0:\n",
      "Training Set Loss: 478.63719309180533 -------- Dev Set Loss: 591.6472769471529\n",
      "Epoch 33 ------------- Batch 200:\n",
      "Training Set Loss: 621.2401626676445 -------- Dev Set Loss: 597.0357862615443\n",
      "Epoch 33 ------------- Batch 400:\n",
      "Training Set Loss: 544.1195344274311 -------- Dev Set Loss: 599.8497363848497\n",
      "Epoch 33 ------------- Batch 600:\n",
      "Training Set Loss: 567.9888223627378 -------- Dev Set Loss: 592.326861171536\n",
      "Epoch 33 ------------- Batch 800:\n",
      "Training Set Loss: 604.150708264993 -------- Dev Set Loss: 601.7226682119249\n",
      "Epoch 34 ------------- Batch 0:\n",
      "Training Set Loss: 475.2290974154964 -------- Dev Set Loss: 591.6014833583536\n",
      "Epoch 34 ------------- Batch 200:\n",
      "Training Set Loss: 618.9893352996208 -------- Dev Set Loss: 597.6870410423696\n",
      "Epoch 34 ------------- Batch 400:\n",
      "Training Set Loss: 541.2962635267073 -------- Dev Set Loss: 600.0189847192252\n",
      "Epoch 34 ------------- Batch 600:\n",
      "Training Set Loss: 562.6185236462177 -------- Dev Set Loss: 592.1347439336936\n",
      "Epoch 34 ------------- Batch 800:\n",
      "Training Set Loss: 601.6828141542994 -------- Dev Set Loss: 602.3200869143063\n",
      "Epoch 35 ------------- Batch 0:\n",
      "Training Set Loss: 472.4428356167294 -------- Dev Set Loss: 591.6536474832157\n",
      "Epoch 35 ------------- Batch 200:\n",
      "Training Set Loss: 617.0801177724645 -------- Dev Set Loss: 597.9920843922906\n",
      "Epoch 35 ------------- Batch 400:\n",
      "Training Set Loss: 538.7674992618298 -------- Dev Set Loss: 600.0543151902237\n",
      "Epoch 35 ------------- Batch 600:\n",
      "Training Set Loss: 557.87628705559 -------- Dev Set Loss: 591.9228524141508\n",
      "Epoch 35 ------------- Batch 800:\n",
      "Training Set Loss: 599.1551468741111 -------- Dev Set Loss: 602.6476257076963\n",
      "Epoch 36 ------------- Batch 0:\n",
      "Training Set Loss: 470.1406957415004 -------- Dev Set Loss: 591.3439506865179\n",
      "Epoch 36 ------------- Batch 200:\n",
      "Training Set Loss: 615.5984218045912 -------- Dev Set Loss: 598.3915401432346\n",
      "Epoch 36 ------------- Batch 400:\n",
      "Training Set Loss: 535.8707198210909 -------- Dev Set Loss: 600.1673773297127\n",
      "Epoch 36 ------------- Batch 600:\n",
      "Training Set Loss: 552.628856934725 -------- Dev Set Loss: 591.891542081866\n",
      "Epoch 36 ------------- Batch 800:\n",
      "Training Set Loss: 596.4371877284588 -------- Dev Set Loss: 603.2544853746721\n",
      "Epoch 37 ------------- Batch 0:\n",
      "Training Set Loss: 467.8633771348425 -------- Dev Set Loss: 591.6275222769331\n",
      "Epoch 37 ------------- Batch 200:\n",
      "Training Set Loss: 614.0104207106469 -------- Dev Set Loss: 598.8953144548805\n",
      "Epoch 37 ------------- Batch 400:\n",
      "Training Set Loss: 533.1731588971003 -------- Dev Set Loss: 600.2182448368035\n",
      "Epoch 37 ------------- Batch 600:\n",
      "Training Set Loss: 548.157544059656 -------- Dev Set Loss: 592.1207121896643\n",
      "Epoch 37 ------------- Batch 800:\n",
      "Training Set Loss: 593.1995013812656 -------- Dev Set Loss: 603.7320191703157\n",
      "Epoch 38 ------------- Batch 0:\n",
      "Training Set Loss: 465.1612621547102 -------- Dev Set Loss: 591.9945713230316\n",
      "Epoch 38 ------------- Batch 200:\n",
      "Training Set Loss: 612.1599296368486 -------- Dev Set Loss: 599.3414771071879\n",
      "Epoch 38 ------------- Batch 400:\n",
      "Training Set Loss: 530.7977575421683 -------- Dev Set Loss: 600.7837680680218\n",
      "Epoch 38 ------------- Batch 600:\n",
      "Training Set Loss: 544.4113502062572 -------- Dev Set Loss: 592.6773896357566\n",
      "Epoch 38 ------------- Batch 800:\n",
      "Training Set Loss: 589.6715010716517 -------- Dev Set Loss: 604.1280590596934\n",
      "Epoch 39 ------------- Batch 0:\n",
      "Training Set Loss: 461.76758572744427 -------- Dev Set Loss: 592.6900148098546\n",
      "Epoch 39 ------------- Batch 200:\n",
      "Training Set Loss: 610.0343794905086 -------- Dev Set Loss: 599.5759865188254\n",
      "Epoch 39 ------------- Batch 400:\n",
      "Training Set Loss: 528.0301961664716 -------- Dev Set Loss: 601.3499675327886\n",
      "Epoch 39 ------------- Batch 600:\n",
      "Training Set Loss: 539.9241641833593 -------- Dev Set Loss: 593.3375431691768\n",
      "Epoch 39 ------------- Batch 800:\n",
      "Training Set Loss: 586.2266812764394 -------- Dev Set Loss: 604.9355760087509\n",
      "Epoch 40 ------------- Batch 0:\n",
      "Training Set Loss: 458.8249279171049 -------- Dev Set Loss: 593.7346627919312\n",
      "Epoch 40 ------------- Batch 200:\n",
      "Training Set Loss: 607.590448669199 -------- Dev Set Loss: 600.3471880080772\n",
      "Epoch 40 ------------- Batch 400:\n",
      "Training Set Loss: 525.5431549680642 -------- Dev Set Loss: 602.3571743349555\n",
      "Epoch 40 ------------- Batch 600:\n",
      "Training Set Loss: 535.6634656231059 -------- Dev Set Loss: 594.2219305047645\n",
      "Epoch 40 ------------- Batch 800:\n",
      "Training Set Loss: 583.4718071365135 -------- Dev Set Loss: 605.8871742370105\n",
      "Epoch 41 ------------- Batch 0:\n",
      "Training Set Loss: 455.4847663895205 -------- Dev Set Loss: 594.8675575810292\n",
      "Epoch 41 ------------- Batch 200:\n",
      "Training Set Loss: 605.1223459954197 -------- Dev Set Loss: 601.1363818776686\n",
      "Epoch 41 ------------- Batch 400:\n",
      "Training Set Loss: 523.1360565946161 -------- Dev Set Loss: 603.5405804005891\n",
      "Epoch 41 ------------- Batch 600:\n",
      "Training Set Loss: 531.175935355642 -------- Dev Set Loss: 595.2876551798\n",
      "Epoch 41 ------------- Batch 800:\n",
      "Training Set Loss: 581.0266243503394 -------- Dev Set Loss: 607.4853991655312\n",
      "Epoch 42 ------------- Batch 0:\n",
      "Training Set Loss: 452.12828366054066 -------- Dev Set Loss: 596.3605522549398\n",
      "Epoch 42 ------------- Batch 200:\n",
      "Training Set Loss: 602.2003664366553 -------- Dev Set Loss: 602.0860051266831\n",
      "Epoch 42 ------------- Batch 400:\n",
      "Training Set Loss: 520.606353387924 -------- Dev Set Loss: 605.0024815331241\n",
      "Epoch 42 ------------- Batch 600:\n",
      "Training Set Loss: 526.4924713426003 -------- Dev Set Loss: 596.3687544749101\n",
      "Epoch 42 ------------- Batch 800:\n",
      "Training Set Loss: 577.8744714501338 -------- Dev Set Loss: 608.4424643584788\n",
      "Epoch 43 ------------- Batch 0:\n",
      "Training Set Loss: 449.50553304655733 -------- Dev Set Loss: 597.189588339194\n",
      "Epoch 43 ------------- Batch 200:\n",
      "Training Set Loss: 598.5925125658453 -------- Dev Set Loss: 602.8235751357553\n",
      "Epoch 43 ------------- Batch 400:\n",
      "Training Set Loss: 517.9438403904603 -------- Dev Set Loss: 606.1230952234703\n",
      "Epoch 43 ------------- Batch 600:\n",
      "Training Set Loss: 522.478558120003 -------- Dev Set Loss: 596.896590097443\n",
      "Epoch 43 ------------- Batch 800:\n",
      "Training Set Loss: 574.1090694702532 -------- Dev Set Loss: 609.1398955953897\n",
      "Epoch 44 ------------- Batch 0:\n",
      "Training Set Loss: 447.14447196920037 -------- Dev Set Loss: 598.2202695691027\n",
      "Epoch 44 ------------- Batch 200:\n",
      "Training Set Loss: 595.3560504193408 -------- Dev Set Loss: 603.4445071683886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 ------------- Batch 400:\n",
      "Training Set Loss: 515.4152374434318 -------- Dev Set Loss: 607.3706320131062\n",
      "Epoch 44 ------------- Batch 600:\n",
      "Training Set Loss: 519.338946661002 -------- Dev Set Loss: 597.7897142018317\n",
      "Epoch 44 ------------- Batch 800:\n",
      "Training Set Loss: 569.8237013476454 -------- Dev Set Loss: 610.2773335841861\n",
      "Epoch 45 ------------- Batch 0:\n",
      "Training Set Loss: 444.49942856043526 -------- Dev Set Loss: 599.2478077063222\n",
      "Epoch 45 ------------- Batch 200:\n",
      "Training Set Loss: 591.8070580714433 -------- Dev Set Loss: 603.9788728002268\n",
      "Epoch 45 ------------- Batch 400:\n",
      "Training Set Loss: 512.0590959926931 -------- Dev Set Loss: 608.9468468726457\n",
      "Epoch 45 ------------- Batch 600:\n",
      "Training Set Loss: 516.0598614614144 -------- Dev Set Loss: 598.8637350548222\n",
      "Epoch 45 ------------- Batch 800:\n",
      "Training Set Loss: 564.9541921830794 -------- Dev Set Loss: 611.8127602096253\n",
      "Epoch 46 ------------- Batch 0:\n",
      "Training Set Loss: 441.4530758667587 -------- Dev Set Loss: 600.3999100613036\n",
      "Epoch 46 ------------- Batch 200:\n",
      "Training Set Loss: 588.083697109515 -------- Dev Set Loss: 604.586079035258\n",
      "Epoch 46 ------------- Batch 400:\n",
      "Training Set Loss: 508.64598607578284 -------- Dev Set Loss: 610.8155669811891\n",
      "Epoch 46 ------------- Batch 600:\n",
      "Training Set Loss: 513.2928885668172 -------- Dev Set Loss: 600.3905902545408\n",
      "Epoch 46 ------------- Batch 800:\n",
      "Training Set Loss: 559.9849515037513 -------- Dev Set Loss: 613.026821584397\n",
      "Epoch 47 ------------- Batch 0:\n",
      "Training Set Loss: 438.41335440853413 -------- Dev Set Loss: 601.5986285762317\n",
      "Epoch 47 ------------- Batch 200:\n",
      "Training Set Loss: 583.1909551910182 -------- Dev Set Loss: 605.09146427826\n",
      "Epoch 47 ------------- Batch 400:\n",
      "Training Set Loss: 506.04027162050716 -------- Dev Set Loss: 612.3328890936704\n",
      "Epoch 47 ------------- Batch 600:\n",
      "Training Set Loss: 509.57112395017504 -------- Dev Set Loss: 601.8575139154226\n",
      "Epoch 47 ------------- Batch 800:\n",
      "Training Set Loss: 555.0298425650914 -------- Dev Set Loss: 614.4908035093863\n",
      "Epoch 48 ------------- Batch 0:\n",
      "Training Set Loss: 435.46998164269417 -------- Dev Set Loss: 603.362550680382\n",
      "Epoch 48 ------------- Batch 200:\n",
      "Training Set Loss: 577.2420330742502 -------- Dev Set Loss: 606.1018292157544\n",
      "Epoch 48 ------------- Batch 400:\n",
      "Training Set Loss: 503.26668510512525 -------- Dev Set Loss: 614.6057012013616\n",
      "Epoch 48 ------------- Batch 600:\n",
      "Training Set Loss: 505.89271740298227 -------- Dev Set Loss: 603.4233877696719\n",
      "Epoch 48 ------------- Batch 800:\n",
      "Training Set Loss: 549.7652228088432 -------- Dev Set Loss: 616.1947771641815\n",
      "Epoch 49 ------------- Batch 0:\n",
      "Training Set Loss: 432.2622342011713 -------- Dev Set Loss: 604.9887188142292\n",
      "Epoch 49 ------------- Batch 200:\n",
      "Training Set Loss: 570.1402255697128 -------- Dev Set Loss: 606.6705574810852\n",
      "Epoch 49 ------------- Batch 400:\n",
      "Training Set Loss: 499.0299099462131 -------- Dev Set Loss: 615.8463009860412\n",
      "Epoch 49 ------------- Batch 600:\n",
      "Training Set Loss: 500.75387937828555 -------- Dev Set Loss: 603.8932833200274\n",
      "Epoch 49 ------------- Batch 800:\n",
      "Training Set Loss: 541.3798692644381 -------- Dev Set Loss: 616.3434376317939\n",
      "Epoch 50 ------------- Batch 0:\n",
      "Training Set Loss: 428.8109883954504 -------- Dev Set Loss: 605.5759172270506\n",
      "Epoch 50 ------------- Batch 200:\n",
      "Training Set Loss: 562.3442081740577 -------- Dev Set Loss: 607.1712836736277\n",
      "Epoch 50 ------------- Batch 400:\n",
      "Training Set Loss: 494.94734924251964 -------- Dev Set Loss: 617.3345361453503\n",
      "Epoch 50 ------------- Batch 600:\n",
      "Training Set Loss: 493.8836579416217 -------- Dev Set Loss: 605.0127803690423\n",
      "Epoch 50 ------------- Batch 800:\n",
      "Training Set Loss: 533.4468361693986 -------- Dev Set Loss: 618.5778648330235\n",
      "Epoch 51 ------------- Batch 0:\n",
      "Training Set Loss: 425.31750563444746 -------- Dev Set Loss: 607.4007845097423\n",
      "Epoch 51 ------------- Batch 200:\n",
      "Training Set Loss: 554.2457144590738 -------- Dev Set Loss: 608.5591079886949\n",
      "Epoch 51 ------------- Batch 400:\n",
      "Training Set Loss: 490.1546778992745 -------- Dev Set Loss: 618.8941970051175\n",
      "Epoch 51 ------------- Batch 600:\n",
      "Training Set Loss: 486.67299248699794 -------- Dev Set Loss: 606.6171332889365\n",
      "Epoch 51 ------------- Batch 800:\n",
      "Training Set Loss: 524.6634752635455 -------- Dev Set Loss: 620.851219710118\n",
      "Epoch 52 ------------- Batch 0:\n",
      "Training Set Loss: 420.6558587345546 -------- Dev Set Loss: 609.2135764976227\n",
      "Epoch 52 ------------- Batch 200:\n",
      "Training Set Loss: 545.3118903741404 -------- Dev Set Loss: 610.226666516706\n",
      "Epoch 52 ------------- Batch 400:\n",
      "Training Set Loss: 484.35833747870913 -------- Dev Set Loss: 620.1356742282601\n",
      "Epoch 52 ------------- Batch 600:\n",
      "Training Set Loss: 480.03022604173 -------- Dev Set Loss: 608.8524132529649\n",
      "Epoch 52 ------------- Batch 800:\n",
      "Training Set Loss: 517.82263011093 -------- Dev Set Loss: 622.8902872116618\n",
      "Epoch 53 ------------- Batch 0:\n",
      "Training Set Loss: 414.99220482171677 -------- Dev Set Loss: 610.5556554174478\n",
      "Epoch 53 ------------- Batch 200:\n",
      "Training Set Loss: 537.3140208529435 -------- Dev Set Loss: 610.9645687517371\n",
      "Epoch 53 ------------- Batch 400:\n",
      "Training Set Loss: 478.77968265819317 -------- Dev Set Loss: 621.4324017707007\n",
      "Epoch 53 ------------- Batch 600:\n",
      "Training Set Loss: 474.32937804740857 -------- Dev Set Loss: 609.4155302923316\n",
      "Epoch 53 ------------- Batch 800:\n",
      "Training Set Loss: 509.45614107461176 -------- Dev Set Loss: 623.7891302709552\n",
      "Epoch 54 ------------- Batch 0:\n",
      "Training Set Loss: 409.0452511680504 -------- Dev Set Loss: 610.4063603370304\n",
      "Epoch 54 ------------- Batch 200:\n",
      "Training Set Loss: 529.4031055140422 -------- Dev Set Loss: 611.0172931456862\n",
      "Epoch 54 ------------- Batch 400:\n",
      "Training Set Loss: 472.58131186106544 -------- Dev Set Loss: 621.9979812323903\n",
      "Epoch 54 ------------- Batch 600:\n",
      "Training Set Loss: 468.78330612823646 -------- Dev Set Loss: 609.4627541651371\n",
      "Epoch 54 ------------- Batch 800:\n",
      "Training Set Loss: 499.60864884508226 -------- Dev Set Loss: 623.9233680770606\n",
      "Epoch 55 ------------- Batch 0:\n",
      "Training Set Loss: 403.171303645114 -------- Dev Set Loss: 610.2160911683424\n",
      "Epoch 55 ------------- Batch 200:\n",
      "Training Set Loss: 522.4499666085242 -------- Dev Set Loss: 611.0141745598308\n",
      "Epoch 55 ------------- Batch 400:\n",
      "Training Set Loss: 466.53811563454735 -------- Dev Set Loss: 622.4477578797089\n",
      "Epoch 55 ------------- Batch 600:\n",
      "Training Set Loss: 462.65081368714453 -------- Dev Set Loss: 610.2024974496564\n",
      "Epoch 55 ------------- Batch 800:\n",
      "Training Set Loss: 488.7581525168159 -------- Dev Set Loss: 623.4364191838489\n",
      "Epoch 56 ------------- Batch 0:\n",
      "Training Set Loss: 396.425079714032 -------- Dev Set Loss: 610.3190879829813\n",
      "Epoch 56 ------------- Batch 200:\n",
      "Training Set Loss: 514.7567303227077 -------- Dev Set Loss: 612.1723264080518\n",
      "Epoch 56 ------------- Batch 400:\n",
      "Training Set Loss: 459.3936049030294 -------- Dev Set Loss: 622.5782963878422\n",
      "Epoch 56 ------------- Batch 600:\n",
      "Training Set Loss: 456.5957726628022 -------- Dev Set Loss: 610.8272461515991\n",
      "Epoch 56 ------------- Batch 800:\n",
      "Training Set Loss: 478.7483820623746 -------- Dev Set Loss: 623.0857265405438\n",
      "Epoch 57 ------------- Batch 0:\n",
      "Training Set Loss: 390.2019364166787 -------- Dev Set Loss: 610.9479765320608\n",
      "Epoch 57 ------------- Batch 200:\n",
      "Training Set Loss: 507.78689815445705 -------- Dev Set Loss: 613.2175297817804\n",
      "Epoch 57 ------------- Batch 400:\n",
      "Training Set Loss: 451.7566656499512 -------- Dev Set Loss: 624.2608253499229\n",
      "Epoch 57 ------------- Batch 600:\n",
      "Training Set Loss: 449.4856527725605 -------- Dev Set Loss: 611.8822497651516\n",
      "Epoch 57 ------------- Batch 800:\n",
      "Training Set Loss: 468.46304387427693 -------- Dev Set Loss: 624.755973011865\n",
      "Epoch 58 ------------- Batch 0:\n",
      "Training Set Loss: 384.7319276816856 -------- Dev Set Loss: 612.4001658668988\n",
      "Epoch 58 ------------- Batch 200:\n",
      "Training Set Loss: 498.9902726486349 -------- Dev Set Loss: 615.0068614513502\n",
      "Epoch 58 ------------- Batch 400:\n",
      "Training Set Loss: 443.29203719313296 -------- Dev Set Loss: 626.6259068712703\n",
      "Epoch 58 ------------- Batch 600:\n",
      "Training Set Loss: 442.0523817235271 -------- Dev Set Loss: 613.811881962967\n",
      "Epoch 58 ------------- Batch 800:\n",
      "Training Set Loss: 456.6352591743113 -------- Dev Set Loss: 626.4880979671921\n",
      "Epoch 59 ------------- Batch 0:\n",
      "Training Set Loss: 377.9196236659842 -------- Dev Set Loss: 613.7329232419736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 ------------- Batch 200:\n",
      "Training Set Loss: 488.4851889168496 -------- Dev Set Loss: 617.8392655608296\n",
      "Epoch 59 ------------- Batch 400:\n",
      "Training Set Loss: 434.170336925077 -------- Dev Set Loss: 629.8878591193525\n",
      "Epoch 59 ------------- Batch 600:\n",
      "Training Set Loss: 433.3551685002159 -------- Dev Set Loss: 616.057173928577\n",
      "Epoch 59 ------------- Batch 800:\n",
      "Training Set Loss: 444.75753284000336 -------- Dev Set Loss: 627.7691734330102\n",
      "Epoch 60 ------------- Batch 0:\n",
      "Training Set Loss: 371.4710213217397 -------- Dev Set Loss: 615.6363875834769\n",
      "Epoch 60 ------------- Batch 200:\n",
      "Training Set Loss: 478.61386575621003 -------- Dev Set Loss: 619.9795777322057\n",
      "Epoch 60 ------------- Batch 400:\n",
      "Training Set Loss: 422.2092054566154 -------- Dev Set Loss: 631.8010954194601\n",
      "Epoch 60 ------------- Batch 600:\n",
      "Training Set Loss: 425.8518053083392 -------- Dev Set Loss: 617.3569118840767\n",
      "Epoch 60 ------------- Batch 800:\n",
      "Training Set Loss: 437.10540496797546 -------- Dev Set Loss: 628.267963880365\n",
      "Epoch 61 ------------- Batch 0:\n",
      "Training Set Loss: 365.7736259638653 -------- Dev Set Loss: 617.6050151196421\n",
      "Epoch 61 ------------- Batch 200:\n",
      "Training Set Loss: 468.3477051524572 -------- Dev Set Loss: 622.6575435037212\n",
      "Epoch 61 ------------- Batch 400:\n",
      "Training Set Loss: 415.13660684783565 -------- Dev Set Loss: 637.2059194708344\n",
      "Epoch 61 ------------- Batch 600:\n",
      "Training Set Loss: 417.12293543260694 -------- Dev Set Loss: 619.6492363240185\n",
      "Epoch 61 ------------- Batch 800:\n",
      "Training Set Loss: 429.04939954741326 -------- Dev Set Loss: 630.774349654467\n",
      "Epoch 62 ------------- Batch 0:\n",
      "Training Set Loss: 359.26281087082623 -------- Dev Set Loss: 620.2970713637756\n",
      "Epoch 62 ------------- Batch 200:\n",
      "Training Set Loss: 458.94922611393025 -------- Dev Set Loss: 628.1798226801302\n",
      "Epoch 62 ------------- Batch 400:\n",
      "Training Set Loss: 408.0335305092515 -------- Dev Set Loss: 640.0103968745308\n",
      "Epoch 62 ------------- Batch 600:\n",
      "Training Set Loss: 409.30550264344527 -------- Dev Set Loss: 623.0198806904417\n",
      "Epoch 62 ------------- Batch 800:\n",
      "Training Set Loss: 421.6401053816 -------- Dev Set Loss: 633.0211203503831\n",
      "Epoch 63 ------------- Batch 0:\n",
      "Training Set Loss: 352.0958611777403 -------- Dev Set Loss: 623.6856567674956\n",
      "Epoch 63 ------------- Batch 200:\n",
      "Training Set Loss: 449.82852474356787 -------- Dev Set Loss: 632.3140639954108\n",
      "Epoch 63 ------------- Batch 400:\n",
      "Training Set Loss: 403.99825012645005 -------- Dev Set Loss: 644.9854949806855\n",
      "Epoch 63 ------------- Batch 600:\n",
      "Training Set Loss: 401.74872542602077 -------- Dev Set Loss: 627.0181188791057\n",
      "Epoch 63 ------------- Batch 800:\n",
      "Training Set Loss: 414.88413368167016 -------- Dev Set Loss: 637.0201556913306\n",
      "Epoch 64 ------------- Batch 0:\n",
      "Training Set Loss: 344.3495415694638 -------- Dev Set Loss: 626.4791098963846\n",
      "Epoch 64 ------------- Batch 200:\n",
      "Training Set Loss: 439.64992216288647 -------- Dev Set Loss: 637.9203101833162\n",
      "Epoch 64 ------------- Batch 400:\n",
      "Training Set Loss: 398.61161622381576 -------- Dev Set Loss: 649.7650497149923\n",
      "Epoch 64 ------------- Batch 600:\n",
      "Training Set Loss: 393.61541383424014 -------- Dev Set Loss: 633.1712637743221\n",
      "Epoch 64 ------------- Batch 800:\n",
      "Training Set Loss: 408.4711510701184 -------- Dev Set Loss: 642.3763396241204\n",
      "Epoch 65 ------------- Batch 0:\n",
      "Training Set Loss: 339.8519418564694 -------- Dev Set Loss: 631.9157689580068\n",
      "Epoch 65 ------------- Batch 200:\n",
      "Training Set Loss: 429.68712428869196 -------- Dev Set Loss: 645.3052594822905\n",
      "Epoch 65 ------------- Batch 400:\n",
      "Training Set Loss: 392.7115100858624 -------- Dev Set Loss: 655.1203318008833\n",
      "Epoch 65 ------------- Batch 600:\n",
      "Training Set Loss: 384.92015040482033 -------- Dev Set Loss: 640.7404432385448\n",
      "Epoch 65 ------------- Batch 800:\n",
      "Training Set Loss: 401.3985751715702 -------- Dev Set Loss: 647.9797481459206\n",
      "Epoch 66 ------------- Batch 0:\n",
      "Training Set Loss: 334.59929380095707 -------- Dev Set Loss: 638.1828976674132\n",
      "Epoch 66 ------------- Batch 200:\n",
      "Training Set Loss: 423.4939794581955 -------- Dev Set Loss: 651.90697714663\n",
      "Epoch 66 ------------- Batch 400:\n",
      "Training Set Loss: 386.7388962626446 -------- Dev Set Loss: 662.4876959769581\n",
      "Epoch 66 ------------- Batch 600:\n",
      "Training Set Loss: 378.2581166662335 -------- Dev Set Loss: 649.1556469056669\n",
      "Epoch 66 ------------- Batch 800:\n",
      "Training Set Loss: 395.81329538815123 -------- Dev Set Loss: 654.707193991568\n",
      "Epoch 67 ------------- Batch 0:\n",
      "Training Set Loss: 330.34537870004806 -------- Dev Set Loss: 646.7578341569597\n",
      "Epoch 67 ------------- Batch 200:\n",
      "Training Set Loss: 417.40282695847816 -------- Dev Set Loss: 659.6674930302162\n",
      "Epoch 67 ------------- Batch 400:\n",
      "Training Set Loss: 380.81382650092 -------- Dev Set Loss: 668.7754615147294\n",
      "Epoch 67 ------------- Batch 600:\n",
      "Training Set Loss: 370.18542064754405 -------- Dev Set Loss: 657.3682405183637\n",
      "Epoch 67 ------------- Batch 800:\n",
      "Training Set Loss: 391.4096840312134 -------- Dev Set Loss: 662.2947809866121\n",
      "Epoch 68 ------------- Batch 0:\n",
      "Training Set Loss: 327.20688194629935 -------- Dev Set Loss: 652.3595219217714\n",
      "Epoch 68 ------------- Batch 200:\n",
      "Training Set Loss: 409.0080866503787 -------- Dev Set Loss: 664.133437885919\n",
      "Epoch 68 ------------- Batch 400:\n",
      "Training Set Loss: 374.5626987324624 -------- Dev Set Loss: 674.485657855387\n",
      "Epoch 68 ------------- Batch 600:\n",
      "Training Set Loss: 361.33746973804523 -------- Dev Set Loss: 664.855108245676\n",
      "Epoch 68 ------------- Batch 800:\n",
      "Training Set Loss: 386.21511120864864 -------- Dev Set Loss: 668.6113445735956\n",
      "Epoch 69 ------------- Batch 0:\n",
      "Training Set Loss: 321.8018033029114 -------- Dev Set Loss: 659.6076684986757\n",
      "Epoch 69 ------------- Batch 200:\n",
      "Training Set Loss: 399.9434978008295 -------- Dev Set Loss: 672.2790921379806\n",
      "Epoch 69 ------------- Batch 400:\n",
      "Training Set Loss: 362.8116828877255 -------- Dev Set Loss: 680.223148586555\n",
      "Epoch 69 ------------- Batch 600:\n",
      "Training Set Loss: 351.53359975243666 -------- Dev Set Loss: 673.098156690569\n",
      "Epoch 69 ------------- Batch 800:\n",
      "Training Set Loss: 377.4735684730556 -------- Dev Set Loss: 673.8254364195006\n",
      "Epoch 70 ------------- Batch 0:\n",
      "Training Set Loss: 317.93063319275365 -------- Dev Set Loss: 667.6425890043444\n",
      "Epoch 70 ------------- Batch 200:\n",
      "Training Set Loss: 390.49503522274625 -------- Dev Set Loss: 680.2618344672318\n",
      "Epoch 70 ------------- Batch 400:\n",
      "Training Set Loss: 341.1555313296045 -------- Dev Set Loss: 688.5161425760907\n",
      "Epoch 70 ------------- Batch 600:\n",
      "Training Set Loss: 336.9669601934155 -------- Dev Set Loss: 683.9195840801559\n",
      "Epoch 70 ------------- Batch 800:\n",
      "Training Set Loss: 365.5440118022334 -------- Dev Set Loss: 685.3754200748383\n",
      "Epoch 71 ------------- Batch 0:\n",
      "Training Set Loss: 308.568728980683 -------- Dev Set Loss: 676.4935431633921\n",
      "Epoch 71 ------------- Batch 200:\n",
      "Training Set Loss: 375.3147299470012 -------- Dev Set Loss: 692.7887659293608\n",
      "Epoch 71 ------------- Batch 400:\n",
      "Training Set Loss: 316.45332621836724 -------- Dev Set Loss: 699.1003353600995\n",
      "Epoch 71 ------------- Batch 600:\n",
      "Training Set Loss: 320.668031882241 -------- Dev Set Loss: 694.8459230697237\n",
      "Epoch 71 ------------- Batch 800:\n",
      "Training Set Loss: 346.34199183069546 -------- Dev Set Loss: 700.6960929042274\n",
      "Epoch 72 ------------- Batch 0:\n",
      "Training Set Loss: 298.4667952731708 -------- Dev Set Loss: 699.9836416901701\n",
      "Epoch 72 ------------- Batch 200:\n",
      "Training Set Loss: 353.02193847474683 -------- Dev Set Loss: 710.1054615769132\n",
      "Epoch 72 ------------- Batch 400:\n",
      "Training Set Loss: 296.2733637753131 -------- Dev Set Loss: 713.6023501698199\n",
      "Epoch 72 ------------- Batch 600:\n",
      "Training Set Loss: 294.69764256989527 -------- Dev Set Loss: 713.1311857191625\n",
      "Epoch 72 ------------- Batch 800:\n",
      "Training Set Loss: 326.98793582066105 -------- Dev Set Loss: 723.7355786837437\n",
      "Epoch 73 ------------- Batch 0:\n",
      "Training Set Loss: 288.46908377251447 -------- Dev Set Loss: 724.9044916700084\n",
      "Epoch 73 ------------- Batch 200:\n",
      "Training Set Loss: 326.72470480134893 -------- Dev Set Loss: 726.9814918279469\n",
      "Epoch 73 ------------- Batch 400:\n",
      "Training Set Loss: 276.6371080985748 -------- Dev Set Loss: 730.8630658377334\n",
      "Epoch 73 ------------- Batch 600:\n",
      "Training Set Loss: 276.2785108706805 -------- Dev Set Loss: 736.2911388062481\n",
      "Epoch 73 ------------- Batch 800:\n",
      "Training Set Loss: 307.73681722722597 -------- Dev Set Loss: 741.8206976136499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 ------------- Batch 0:\n",
      "Training Set Loss: 278.2662928263777 -------- Dev Set Loss: 752.9639272464755\n",
      "Epoch 74 ------------- Batch 200:\n",
      "Training Set Loss: 306.36365997943454 -------- Dev Set Loss: 751.0892050949624\n",
      "Epoch 74 ------------- Batch 400:\n",
      "Training Set Loss: 256.6417405366065 -------- Dev Set Loss: 758.560715742309\n",
      "Epoch 74 ------------- Batch 600:\n",
      "Training Set Loss: 261.93538559643014 -------- Dev Set Loss: 755.6443856451871\n",
      "Epoch 74 ------------- Batch 800:\n",
      "Training Set Loss: 291.7495384831249 -------- Dev Set Loss: 766.4675429050226\n",
      "Epoch 75 ------------- Batch 0:\n",
      "Training Set Loss: 266.3100656359602 -------- Dev Set Loss: 792.6466750924963\n",
      "Epoch 75 ------------- Batch 200:\n",
      "Training Set Loss: 282.35310889558946 -------- Dev Set Loss: 777.5673935745551\n",
      "Epoch 75 ------------- Batch 400:\n",
      "Training Set Loss: 237.99336539632833 -------- Dev Set Loss: 782.7153801955222\n",
      "Epoch 75 ------------- Batch 600:\n",
      "Training Set Loss: 230.6134453526506 -------- Dev Set Loss: 796.2215521938011\n",
      "Epoch 75 ------------- Batch 800:\n",
      "Training Set Loss: 273.04187393839516 -------- Dev Set Loss: 800.9804388009908\n",
      "Epoch 76 ------------- Batch 0:\n",
      "Training Set Loss: 250.58191777656447 -------- Dev Set Loss: 812.2746545637435\n",
      "Epoch 76 ------------- Batch 200:\n",
      "Training Set Loss: 263.60563645824544 -------- Dev Set Loss: 807.1550746675581\n",
      "Epoch 76 ------------- Batch 400:\n",
      "Training Set Loss: 223.17453531429214 -------- Dev Set Loss: 816.4496424054247\n",
      "Epoch 76 ------------- Batch 600:\n",
      "Training Set Loss: 214.1976895532979 -------- Dev Set Loss: 825.903469696668\n",
      "Epoch 76 ------------- Batch 800:\n",
      "Training Set Loss: 257.21487529929607 -------- Dev Set Loss: 840.4511193675738\n",
      "Epoch 77 ------------- Batch 0:\n",
      "Training Set Loss: 229.81925348638413 -------- Dev Set Loss: 827.4012045391687\n",
      "Epoch 77 ------------- Batch 200:\n",
      "Training Set Loss: 244.8366466814263 -------- Dev Set Loss: 840.3997461015138\n",
      "Epoch 77 ------------- Batch 400:\n",
      "Training Set Loss: 212.0645961528437 -------- Dev Set Loss: 859.5761923232178\n",
      "Epoch 77 ------------- Batch 600:\n",
      "Training Set Loss: 199.4147584170389 -------- Dev Set Loss: 853.0971780977121\n",
      "Epoch 77 ------------- Batch 800:\n",
      "Training Set Loss: 242.73075564309943 -------- Dev Set Loss: 876.5701613907112\n",
      "Epoch 78 ------------- Batch 0:\n",
      "Training Set Loss: 212.10111607564573 -------- Dev Set Loss: 849.1088487543748\n",
      "Epoch 78 ------------- Batch 200:\n",
      "Training Set Loss: 230.22629429585618 -------- Dev Set Loss: 869.125033952186\n",
      "Epoch 78 ------------- Batch 400:\n",
      "Training Set Loss: 204.1449364385632 -------- Dev Set Loss: 904.5730152751983\n",
      "Epoch 78 ------------- Batch 600:\n",
      "Training Set Loss: 180.6264415258835 -------- Dev Set Loss: 881.6055169754069\n",
      "Epoch 78 ------------- Batch 800:\n",
      "Training Set Loss: 225.32094070458817 -------- Dev Set Loss: 908.5163709013509\n",
      "Epoch 79 ------------- Batch 0:\n",
      "Training Set Loss: 198.89205875847017 -------- Dev Set Loss: 891.2380748357467\n",
      "Epoch 79 ------------- Batch 200:\n",
      "Training Set Loss: 216.2829359879075 -------- Dev Set Loss: 911.7519643558572\n",
      "Epoch 79 ------------- Batch 400:\n",
      "Training Set Loss: 191.41260024514773 -------- Dev Set Loss: 935.0831612921968\n",
      "Epoch 79 ------------- Batch 600:\n",
      "Training Set Loss: 162.3311271624204 -------- Dev Set Loss: 916.7133601408549\n",
      "Epoch 79 ------------- Batch 800:\n",
      "Training Set Loss: 210.92398507107185 -------- Dev Set Loss: 948.3505187466112\n",
      "Epoch 80 ------------- Batch 0:\n",
      "Training Set Loss: 186.1639683904774 -------- Dev Set Loss: 943.534997686272\n",
      "Epoch 80 ------------- Batch 200:\n",
      "Training Set Loss: 204.19784097185232 -------- Dev Set Loss: 951.8981503645745\n",
      "Epoch 80 ------------- Batch 400:\n",
      "Training Set Loss: 176.83392114418746 -------- Dev Set Loss: 967.3747189430425\n",
      "Epoch 80 ------------- Batch 600:\n",
      "Training Set Loss: 151.07631351392212 -------- Dev Set Loss: 957.60320145174\n",
      "Epoch 80 ------------- Batch 800:\n",
      "Training Set Loss: 196.0712892201584 -------- Dev Set Loss: 980.9885309196651\n",
      "Epoch 81 ------------- Batch 0:\n",
      "Training Set Loss: 174.37784626713596 -------- Dev Set Loss: 991.0865733895783\n",
      "Epoch 81 ------------- Batch 200:\n",
      "Training Set Loss: 190.7821311978551 -------- Dev Set Loss: 990.6483790619496\n",
      "Epoch 81 ------------- Batch 400:\n",
      "Training Set Loss: 169.52975385970896 -------- Dev Set Loss: 1002.4451723491146\n",
      "Epoch 81 ------------- Batch 600:\n",
      "Training Set Loss: 141.3370083538581 -------- Dev Set Loss: 1004.3617186320622\n",
      "Epoch 81 ------------- Batch 800:\n",
      "Training Set Loss: 183.4239655376121 -------- Dev Set Loss: 1012.0923557043042\n",
      "Epoch 82 ------------- Batch 0:\n",
      "Training Set Loss: 162.6276477126578 -------- Dev Set Loss: 1030.183002329692\n",
      "Epoch 82 ------------- Batch 200:\n",
      "Training Set Loss: 178.0251761445839 -------- Dev Set Loss: 1031.4488422231857\n",
      "Epoch 82 ------------- Batch 400:\n",
      "Training Set Loss: 155.2787404531223 -------- Dev Set Loss: 1033.424216662882\n",
      "Epoch 82 ------------- Batch 600:\n",
      "Training Set Loss: 135.43153505969968 -------- Dev Set Loss: 1044.697652135582\n",
      "Epoch 82 ------------- Batch 800:\n",
      "Training Set Loss: 172.18103204620866 -------- Dev Set Loss: 1047.6261261657623\n",
      "Epoch 83 ------------- Batch 0:\n",
      "Training Set Loss: 151.37745567245966 -------- Dev Set Loss: 1058.2242205280884\n",
      "Epoch 83 ------------- Batch 200:\n",
      "Training Set Loss: 167.50905619088797 -------- Dev Set Loss: 1078.1915784225946\n",
      "Epoch 83 ------------- Batch 400:\n",
      "Training Set Loss: 138.34567695944952 -------- Dev Set Loss: 1072.3659000595458\n",
      "Epoch 83 ------------- Batch 600:\n",
      "Training Set Loss: 129.1524301515263 -------- Dev Set Loss: 1084.2693784021797\n",
      "Epoch 83 ------------- Batch 800:\n",
      "Training Set Loss: 164.10685037427504 -------- Dev Set Loss: 1089.3073451601742\n",
      "Epoch 84 ------------- Batch 0:\n",
      "Training Set Loss: 142.63648900123877 -------- Dev Set Loss: 1093.657712595029\n",
      "Epoch 84 ------------- Batch 200:\n",
      "Training Set Loss: 157.44532434477392 -------- Dev Set Loss: 1128.439565424764\n",
      "Epoch 84 ------------- Batch 400:\n",
      "Training Set Loss: 128.8295447709869 -------- Dev Set Loss: 1109.4471866203892\n",
      "Epoch 84 ------------- Batch 600:\n",
      "Training Set Loss: 125.2169850289419 -------- Dev Set Loss: 1132.7446295735954\n",
      "Epoch 84 ------------- Batch 800:\n",
      "Training Set Loss: 158.66441436220575 -------- Dev Set Loss: 1125.0560870195095\n",
      "Epoch 85 ------------- Batch 0:\n",
      "Training Set Loss: 137.0775520669094 -------- Dev Set Loss: 1131.860085427959\n",
      "Epoch 85 ------------- Batch 200:\n",
      "Training Set Loss: 147.62617343804112 -------- Dev Set Loss: 1168.0917810870922\n",
      "Epoch 85 ------------- Batch 400:\n",
      "Training Set Loss: 121.02622617303132 -------- Dev Set Loss: 1137.3876929044395\n",
      "Epoch 85 ------------- Batch 600:\n",
      "Training Set Loss: 119.38558974467296 -------- Dev Set Loss: 1176.1262981300229\n",
      "Epoch 85 ------------- Batch 800:\n",
      "Training Set Loss: 153.30979900720206 -------- Dev Set Loss: 1153.2687815068962\n",
      "Epoch 86 ------------- Batch 0:\n",
      "Training Set Loss: 131.95503707012466 -------- Dev Set Loss: 1167.362870051544\n",
      "Epoch 86 ------------- Batch 200:\n",
      "Training Set Loss: 136.31958637051014 -------- Dev Set Loss: 1200.789691929411\n",
      "Epoch 86 ------------- Batch 400:\n",
      "Training Set Loss: 113.1752062412572 -------- Dev Set Loss: 1164.5826804110325\n",
      "Epoch 86 ------------- Batch 600:\n",
      "Training Set Loss: 110.02233649134124 -------- Dev Set Loss: 1215.9025971559647\n",
      "Epoch 86 ------------- Batch 800:\n",
      "Training Set Loss: 143.7347227959301 -------- Dev Set Loss: 1180.6576972922853\n",
      "Epoch 87 ------------- Batch 0:\n",
      "Training Set Loss: 126.88836867787454 -------- Dev Set Loss: 1202.7508532442223\n",
      "Epoch 87 ------------- Batch 200:\n",
      "Training Set Loss: 127.1570395385932 -------- Dev Set Loss: 1231.3535920638874\n",
      "Epoch 87 ------------- Batch 400:\n",
      "Training Set Loss: 108.82496538721898 -------- Dev Set Loss: 1196.2323899065414\n",
      "Epoch 87 ------------- Batch 600:\n",
      "Training Set Loss: 107.59242766302526 -------- Dev Set Loss: 1243.024882483954\n",
      "Epoch 87 ------------- Batch 800:\n",
      "Training Set Loss: 128.4681383384713 -------- Dev Set Loss: 1212.7710691397454\n",
      "Epoch 88 ------------- Batch 0:\n",
      "Training Set Loss: 120.9655379612731 -------- Dev Set Loss: 1236.944574511802\n",
      "Epoch 88 ------------- Batch 200:\n",
      "Training Set Loss: 128.07011879221858 -------- Dev Set Loss: 1255.6064031270314\n",
      "Epoch 88 ------------- Batch 400:\n",
      "Training Set Loss: 112.23971900581066 -------- Dev Set Loss: 1228.66998036328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 ------------- Batch 600:\n",
      "Training Set Loss: 105.46025805774536 -------- Dev Set Loss: 1267.226709591231\n",
      "Epoch 88 ------------- Batch 800:\n",
      "Training Set Loss: 121.19060466415468 -------- Dev Set Loss: 1252.5578778164208\n",
      "Epoch 89 ------------- Batch 0:\n",
      "Training Set Loss: 116.48455825031664 -------- Dev Set Loss: 1282.1753546242219\n",
      "Epoch 89 ------------- Batch 200:\n",
      "Training Set Loss: 132.374713218559 -------- Dev Set Loss: 1270.7429327614147\n",
      "Epoch 89 ------------- Batch 400:\n",
      "Training Set Loss: 109.78767477046527 -------- Dev Set Loss: 1261.2797495557857\n",
      "Epoch 89 ------------- Batch 600:\n",
      "Training Set Loss: 103.84336863986469 -------- Dev Set Loss: 1294.4329367664827\n",
      "Epoch 89 ------------- Batch 800:\n",
      "Training Set Loss: 114.9351060904679 -------- Dev Set Loss: 1286.3065480485377\n",
      "Epoch 90 ------------- Batch 0:\n",
      "Training Set Loss: 110.76060373126796 -------- Dev Set Loss: 1307.3821672671952\n",
      "Epoch 90 ------------- Batch 200:\n",
      "Training Set Loss: 125.57775043410517 -------- Dev Set Loss: 1285.1011308197615\n",
      "Epoch 90 ------------- Batch 400:\n",
      "Training Set Loss: 101.06910917210254 -------- Dev Set Loss: 1297.138204258065\n",
      "Epoch 90 ------------- Batch 600:\n",
      "Training Set Loss: 100.75901450535446 -------- Dev Set Loss: 1310.857123130838\n",
      "Epoch 90 ------------- Batch 800:\n",
      "Training Set Loss: 108.6388568347026 -------- Dev Set Loss: 1315.2952224446565\n",
      "Epoch 91 ------------- Batch 0:\n",
      "Training Set Loss: 105.57338411614339 -------- Dev Set Loss: 1331.2562833967474\n",
      "Epoch 91 ------------- Batch 200:\n",
      "Training Set Loss: 115.79931578494154 -------- Dev Set Loss: 1300.547710056561\n",
      "Epoch 91 ------------- Batch 400:\n",
      "Training Set Loss: 90.35301218844265 -------- Dev Set Loss: 1333.5502479513568\n",
      "Epoch 91 ------------- Batch 600:\n",
      "Training Set Loss: 92.1668145612908 -------- Dev Set Loss: 1326.1841772190257\n",
      "Epoch 91 ------------- Batch 800:\n",
      "Training Set Loss: 104.05308115678308 -------- Dev Set Loss: 1343.2382389961188\n",
      "Epoch 92 ------------- Batch 0:\n",
      "Training Set Loss: 97.92002175339637 -------- Dev Set Loss: 1355.1343889957768\n",
      "Epoch 92 ------------- Batch 200:\n",
      "Training Set Loss: 103.83565711766434 -------- Dev Set Loss: 1322.946378817924\n",
      "Epoch 92 ------------- Batch 400:\n",
      "Training Set Loss: 83.21444334952602 -------- Dev Set Loss: 1360.1788914849799\n",
      "Epoch 92 ------------- Batch 600:\n",
      "Training Set Loss: 90.27564608924364 -------- Dev Set Loss: 1350.396992388532\n",
      "Epoch 92 ------------- Batch 800:\n",
      "Training Set Loss: 97.86396837749203 -------- Dev Set Loss: 1364.8377022792376\n",
      "Epoch 93 ------------- Batch 0:\n",
      "Training Set Loss: 93.72415620758278 -------- Dev Set Loss: 1385.9451127876953\n",
      "Epoch 93 ------------- Batch 200:\n",
      "Training Set Loss: 96.77911145694682 -------- Dev Set Loss: 1345.933456313541\n",
      "Epoch 93 ------------- Batch 400:\n",
      "Training Set Loss: 78.03792098062702 -------- Dev Set Loss: 1384.817120838831\n",
      "Epoch 93 ------------- Batch 600:\n",
      "Training Set Loss: 88.52949449413357 -------- Dev Set Loss: 1372.1209437298114\n",
      "Epoch 93 ------------- Batch 800:\n",
      "Training Set Loss: 90.19736492231286 -------- Dev Set Loss: 1388.8078720851147\n",
      "Epoch 94 ------------- Batch 0:\n",
      "Training Set Loss: 92.77300300350254 -------- Dev Set Loss: 1409.762497519623\n",
      "Epoch 94 ------------- Batch 200:\n",
      "Training Set Loss: 94.30149143388829 -------- Dev Set Loss: 1363.5551524619489\n",
      "Epoch 94 ------------- Batch 400:\n",
      "Training Set Loss: 73.34970358067093 -------- Dev Set Loss: 1409.4366440118415\n",
      "Epoch 94 ------------- Batch 600:\n",
      "Training Set Loss: 84.65684130102473 -------- Dev Set Loss: 1396.1357021085332\n",
      "Epoch 94 ------------- Batch 800:\n",
      "Training Set Loss: 84.852005327755 -------- Dev Set Loss: 1418.0547552089297\n",
      "Epoch 95 ------------- Batch 0:\n",
      "Training Set Loss: 87.90785297456077 -------- Dev Set Loss: 1431.5013151874357\n",
      "Epoch 95 ------------- Batch 200:\n",
      "Training Set Loss: 86.53922540775486 -------- Dev Set Loss: 1381.4169692412045\n",
      "Epoch 95 ------------- Batch 400:\n",
      "Training Set Loss: 70.06313973197769 -------- Dev Set Loss: 1435.9193023149362\n",
      "Epoch 95 ------------- Batch 600:\n",
      "Training Set Loss: 80.09759068123076 -------- Dev Set Loss: 1418.3661915231146\n",
      "Epoch 95 ------------- Batch 800:\n",
      "Training Set Loss: 79.60487349441735 -------- Dev Set Loss: 1447.6218856625583\n",
      "Epoch 96 ------------- Batch 0:\n",
      "Training Set Loss: 80.44571719330332 -------- Dev Set Loss: 1460.5606847733873\n",
      "Epoch 96 ------------- Batch 200:\n",
      "Training Set Loss: 76.56488064687771 -------- Dev Set Loss: 1407.4798953020927\n",
      "Epoch 96 ------------- Batch 400:\n",
      "Training Set Loss: 67.09863808077907 -------- Dev Set Loss: 1457.7872252492898\n",
      "Epoch 96 ------------- Batch 600:\n",
      "Training Set Loss: 72.70672171519422 -------- Dev Set Loss: 1443.6169078358637\n",
      "Epoch 96 ------------- Batch 800:\n",
      "Training Set Loss: 76.39270697402611 -------- Dev Set Loss: 1476.6029384861106\n",
      "Epoch 97 ------------- Batch 0:\n",
      "Training Set Loss: 80.52141265744415 -------- Dev Set Loss: 1490.0809607483257\n",
      "Epoch 97 ------------- Batch 200:\n",
      "Training Set Loss: 76.17913572564787 -------- Dev Set Loss: 1427.7158141125449\n",
      "Epoch 97 ------------- Batch 400:\n",
      "Training Set Loss: 68.77522791580289 -------- Dev Set Loss: 1480.884445850615\n",
      "Epoch 97 ------------- Batch 600:\n",
      "Training Set Loss: 69.41730190461553 -------- Dev Set Loss: 1471.2370093792206\n",
      "Epoch 97 ------------- Batch 800:\n",
      "Training Set Loss: 71.73668334029988 -------- Dev Set Loss: 1504.60172139588\n",
      "Epoch 98 ------------- Batch 0:\n",
      "Training Set Loss: 77.70846218502146 -------- Dev Set Loss: 1500.5766921516627\n",
      "Epoch 98 ------------- Batch 200:\n",
      "Training Set Loss: 74.07905989621874 -------- Dev Set Loss: 1445.32067079155\n",
      "Epoch 98 ------------- Batch 400:\n",
      "Training Set Loss: 76.78788960486867 -------- Dev Set Loss: 1513.2207861133334\n",
      "Epoch 98 ------------- Batch 600:\n",
      "Training Set Loss: 67.16496488540395 -------- Dev Set Loss: 1489.4807152252226\n",
      "Epoch 98 ------------- Batch 800:\n",
      "Training Set Loss: 67.68748195858844 -------- Dev Set Loss: 1534.2314794578342\n",
      "Epoch 99 ------------- Batch 0:\n",
      "Training Set Loss: 72.58258885337207 -------- Dev Set Loss: 1524.0549932314375\n",
      "Epoch 99 ------------- Batch 200:\n",
      "Training Set Loss: 60.9375067586518 -------- Dev Set Loss: 1474.1760309098834\n",
      "Epoch 99 ------------- Batch 400:\n",
      "Training Set Loss: 79.98881960273931 -------- Dev Set Loss: 1548.5668770212694\n",
      "Epoch 99 ------------- Batch 600:\n",
      "Training Set Loss: 62.847249537768235 -------- Dev Set Loss: 1504.6311814056762\n",
      "Epoch 99 ------------- Batch 800:\n",
      "Training Set Loss: 64.43054064048151 -------- Dev Set Loss: 1552.3624553462628\n",
      "Epoch 100 ------------- Batch 0:\n",
      "Training Set Loss: 66.3309450056268 -------- Dev Set Loss: 1546.227202103664\n",
      "Epoch 100 ------------- Batch 200:\n",
      "Training Set Loss: 54.99208938823097 -------- Dev Set Loss: 1499.3197741038935\n",
      "Epoch 100 ------------- Batch 400:\n",
      "Training Set Loss: 77.94511328371635 -------- Dev Set Loss: 1580.6470940328954\n",
      "Epoch 100 ------------- Batch 600:\n",
      "Training Set Loss: 57.686676421455246 -------- Dev Set Loss: 1527.507067378633\n",
      "Epoch 100 ------------- Batch 800:\n",
      "Training Set Loss: 63.459787186936936 -------- Dev Set Loss: 1573.1980053607872\n",
      "Epoch 101 ------------- Batch 0:\n",
      "Training Set Loss: 64.12271745770421 -------- Dev Set Loss: 1575.1581177230496\n",
      "Epoch 101 ------------- Batch 200:\n",
      "Training Set Loss: 52.872897500560825 -------- Dev Set Loss: 1524.0828112633849\n",
      "Epoch 101 ------------- Batch 400:\n",
      "Training Set Loss: 74.22207968584712 -------- Dev Set Loss: 1609.8220032250038\n",
      "Epoch 101 ------------- Batch 600:\n",
      "Training Set Loss: 53.92058379417169 -------- Dev Set Loss: 1552.089358424603\n",
      "Epoch 101 ------------- Batch 800:\n",
      "Training Set Loss: 62.95842927689864 -------- Dev Set Loss: 1586.2489813406035\n",
      "Epoch 102 ------------- Batch 0:\n",
      "Training Set Loss: 65.34554799469957 -------- Dev Set Loss: 1596.558014881245\n",
      "Epoch 102 ------------- Batch 200:\n",
      "Training Set Loss: 58.02102282271383 -------- Dev Set Loss: 1552.721978504464\n",
      "Epoch 102 ------------- Batch 400:\n",
      "Training Set Loss: 68.47873497286857 -------- Dev Set Loss: 1642.3970942727706\n",
      "Epoch 102 ------------- Batch 600:\n",
      "Training Set Loss: 52.04116375668098 -------- Dev Set Loss: 1576.9885236621283\n",
      "Epoch 102 ------------- Batch 800:\n",
      "Training Set Loss: 60.123204247653206 -------- Dev Set Loss: 1598.4372949888218\n",
      "Epoch 103 ------------- Batch 0:\n",
      "Training Set Loss: 68.34068390019797 -------- Dev Set Loss: 1606.4193675545434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103 ------------- Batch 200:\n",
      "Training Set Loss: 58.59474200826256 -------- Dev Set Loss: 1582.3263174457759\n",
      "Epoch 103 ------------- Batch 400:\n",
      "Training Set Loss: 62.83995751931464 -------- Dev Set Loss: 1668.7405341840479\n",
      "Epoch 103 ------------- Batch 600:\n",
      "Training Set Loss: 50.53595498966876 -------- Dev Set Loss: 1599.4395776160582\n",
      "Epoch 103 ------------- Batch 800:\n",
      "Training Set Loss: 56.065256749855365 -------- Dev Set Loss: 1615.3218994572817\n",
      "Epoch 104 ------------- Batch 0:\n",
      "Training Set Loss: 67.71518011745407 -------- Dev Set Loss: 1621.5594808131575\n",
      "Epoch 104 ------------- Batch 200:\n",
      "Training Set Loss: 57.531572331662126 -------- Dev Set Loss: 1607.311840809328\n",
      "Epoch 104 ------------- Batch 400:\n",
      "Training Set Loss: 56.650317022259614 -------- Dev Set Loss: 1689.789343100876\n",
      "Epoch 104 ------------- Batch 600:\n",
      "Training Set Loss: 50.56558289857131 -------- Dev Set Loss: 1617.7815962074194\n",
      "Epoch 104 ------------- Batch 800:\n",
      "Training Set Loss: 51.945883262674286 -------- Dev Set Loss: 1635.1859695313176\n",
      "Epoch 105 ------------- Batch 0:\n",
      "Training Set Loss: 66.77309673913618 -------- Dev Set Loss: 1639.4900271733325\n",
      "Epoch 105 ------------- Batch 200:\n",
      "Training Set Loss: 56.65791892789777 -------- Dev Set Loss: 1636.3551632584956\n",
      "Epoch 105 ------------- Batch 400:\n",
      "Training Set Loss: 48.2017554517673 -------- Dev Set Loss: 1699.9575299727073\n",
      "Epoch 105 ------------- Batch 600:\n",
      "Training Set Loss: 53.58414152770642 -------- Dev Set Loss: 1642.5199003191126\n",
      "Epoch 105 ------------- Batch 800:\n",
      "Training Set Loss: 48.08933512255092 -------- Dev Set Loss: 1655.526216741057\n",
      "Epoch 106 ------------- Batch 0:\n",
      "Training Set Loss: 65.27587227882736 -------- Dev Set Loss: 1650.9119478500636\n",
      "Epoch 106 ------------- Batch 200:\n",
      "Training Set Loss: 56.90734784125135 -------- Dev Set Loss: 1665.87863985814\n",
      "Epoch 106 ------------- Batch 400:\n",
      "Training Set Loss: 44.19839638251503 -------- Dev Set Loss: 1715.743336676408\n",
      "Epoch 106 ------------- Batch 600:\n",
      "Training Set Loss: 54.95298979317434 -------- Dev Set Loss: 1673.811280988799\n",
      "Epoch 106 ------------- Batch 800:\n",
      "Training Set Loss: 45.36215641837076 -------- Dev Set Loss: 1673.809894884935\n",
      "Epoch 107 ------------- Batch 0:\n",
      "Training Set Loss: 65.33291726668082 -------- Dev Set Loss: 1667.0737646790847\n",
      "Epoch 107 ------------- Batch 200:\n",
      "Training Set Loss: 53.14439411551479 -------- Dev Set Loss: 1691.9680506824232\n",
      "Epoch 107 ------------- Batch 400:\n",
      "Training Set Loss: 40.990677368972776 -------- Dev Set Loss: 1734.6438647339724\n",
      "Epoch 107 ------------- Batch 600:\n",
      "Training Set Loss: 50.916111624397004 -------- Dev Set Loss: 1706.203801882692\n",
      "Epoch 107 ------------- Batch 800:\n",
      "Training Set Loss: 42.98493510383356 -------- Dev Set Loss: 1688.3155707019564\n",
      "Epoch 108 ------------- Batch 0:\n",
      "Training Set Loss: 65.59123503275455 -------- Dev Set Loss: 1680.1642493872164\n",
      "Epoch 108 ------------- Batch 200:\n",
      "Training Set Loss: 47.67747643675911 -------- Dev Set Loss: 1716.4136315610447\n",
      "Epoch 108 ------------- Batch 400:\n",
      "Training Set Loss: 38.05077150888144 -------- Dev Set Loss: 1747.1965479730102\n",
      "Epoch 108 ------------- Batch 600:\n",
      "Training Set Loss: 45.203011448165235 -------- Dev Set Loss: 1736.8688545404812\n",
      "Epoch 108 ------------- Batch 800:\n",
      "Training Set Loss: 40.95153130390778 -------- Dev Set Loss: 1704.784480667057\n",
      "Epoch 109 ------------- Batch 0:\n",
      "Training Set Loss: 68.29655127123894 -------- Dev Set Loss: 1690.2248915923683\n",
      "Epoch 109 ------------- Batch 200:\n",
      "Training Set Loss: 40.95169087255316 -------- Dev Set Loss: 1732.0036720212477\n",
      "Epoch 109 ------------- Batch 400:\n",
      "Training Set Loss: 36.77248870978176 -------- Dev Set Loss: 1757.528711457009\n",
      "Epoch 109 ------------- Batch 600:\n",
      "Training Set Loss: 38.904347800720544 -------- Dev Set Loss: 1751.8188271912034\n",
      "Epoch 109 ------------- Batch 800:\n",
      "Training Set Loss: 39.07888605233745 -------- Dev Set Loss: 1715.5888076945453\n",
      "Epoch 110 ------------- Batch 0:\n",
      "Training Set Loss: 63.543651332893084 -------- Dev Set Loss: 1703.4725307911867\n",
      "Epoch 110 ------------- Batch 200:\n",
      "Training Set Loss: 39.22160871682638 -------- Dev Set Loss: 1742.9785274721703\n",
      "Epoch 110 ------------- Batch 400:\n",
      "Training Set Loss: 35.91930968505635 -------- Dev Set Loss: 1771.74290749186\n",
      "Epoch 110 ------------- Batch 600:\n",
      "Training Set Loss: 36.05996487992091 -------- Dev Set Loss: 1764.5194301982397\n",
      "Epoch 110 ------------- Batch 800:\n",
      "Training Set Loss: 36.879121174443 -------- Dev Set Loss: 1730.596616636422\n",
      "Epoch 111 ------------- Batch 0:\n",
      "Training Set Loss: 59.034857680541606 -------- Dev Set Loss: 1720.8130408153108\n",
      "Epoch 111 ------------- Batch 200:\n",
      "Training Set Loss: 36.94280846487476 -------- Dev Set Loss: 1754.621597843115\n",
      "Epoch 111 ------------- Batch 400:\n",
      "Training Set Loss: 33.90535506412057 -------- Dev Set Loss: 1783.4687709009295\n",
      "Epoch 111 ------------- Batch 600:\n",
      "Training Set Loss: 33.83599589472156 -------- Dev Set Loss: 1778.5955603213752\n",
      "Epoch 111 ------------- Batch 800:\n",
      "Training Set Loss: 35.2271092180073 -------- Dev Set Loss: 1743.53564590139\n",
      "Epoch 112 ------------- Batch 0:\n",
      "Training Set Loss: 54.47083861738089 -------- Dev Set Loss: 1735.213224063609\n",
      "Epoch 112 ------------- Batch 200:\n",
      "Training Set Loss: 36.290424514430974 -------- Dev Set Loss: 1768.5314398586763\n",
      "Epoch 112 ------------- Batch 400:\n",
      "Training Set Loss: 33.18683518331334 -------- Dev Set Loss: 1795.1712851184598\n",
      "Epoch 112 ------------- Batch 600:\n",
      "Training Set Loss: 31.772959730138613 -------- Dev Set Loss: 1786.6907145592427\n",
      "Epoch 112 ------------- Batch 800:\n",
      "Training Set Loss: 33.50875016416579 -------- Dev Set Loss: 1754.374858257799\n",
      "Epoch 113 ------------- Batch 0:\n",
      "Training Set Loss: 47.420081002806626 -------- Dev Set Loss: 1748.510655546957\n",
      "Epoch 113 ------------- Batch 200:\n",
      "Training Set Loss: 36.03164514182932 -------- Dev Set Loss: 1782.2030105142223\n",
      "Epoch 113 ------------- Batch 400:\n",
      "Training Set Loss: 34.290678027941574 -------- Dev Set Loss: 1808.3304834410778\n",
      "Epoch 113 ------------- Batch 600:\n",
      "Training Set Loss: 30.29780995843784 -------- Dev Set Loss: 1794.3416704412455\n",
      "Epoch 113 ------------- Batch 800:\n",
      "Training Set Loss: 31.810220797864613 -------- Dev Set Loss: 1765.2122633797876\n",
      "Epoch 114 ------------- Batch 0:\n",
      "Training Set Loss: 42.569776610944956 -------- Dev Set Loss: 1762.5358557899349\n",
      "Epoch 114 ------------- Batch 200:\n",
      "Training Set Loss: 35.125706391875575 -------- Dev Set Loss: 1795.2824586407387\n",
      "Epoch 114 ------------- Batch 400:\n",
      "Training Set Loss: 35.554800086567646 -------- Dev Set Loss: 1816.3468760937646\n",
      "Epoch 114 ------------- Batch 600:\n",
      "Training Set Loss: 29.185586379138428 -------- Dev Set Loss: 1805.689561681517\n",
      "Epoch 114 ------------- Batch 800:\n",
      "Training Set Loss: 32.18536966604087 -------- Dev Set Loss: 1783.6307875310488\n",
      "Epoch 115 ------------- Batch 0:\n",
      "Training Set Loss: 38.52020014891568 -------- Dev Set Loss: 1773.9749939168412\n",
      "Epoch 115 ------------- Batch 200:\n",
      "Training Set Loss: 33.612909156901225 -------- Dev Set Loss: 1810.5320635786907\n",
      "Epoch 115 ------------- Batch 400:\n",
      "Training Set Loss: 37.0765789149348 -------- Dev Set Loss: 1824.4064370500114\n",
      "Epoch 115 ------------- Batch 600:\n",
      "Training Set Loss: 28.38278487208348 -------- Dev Set Loss: 1818.3267123646956\n",
      "Epoch 115 ------------- Batch 800:\n",
      "Training Set Loss: 33.11716227586151 -------- Dev Set Loss: 1806.5441113660693\n",
      "Epoch 116 ------------- Batch 0:\n",
      "Training Set Loss: 34.34705240093582 -------- Dev Set Loss: 1788.1830243970676\n",
      "Epoch 116 ------------- Batch 200:\n",
      "Training Set Loss: 31.450426528421374 -------- Dev Set Loss: 1825.044585793095\n",
      "Epoch 116 ------------- Batch 400:\n",
      "Training Set Loss: 37.13269657198759 -------- Dev Set Loss: 1831.501601477688\n",
      "Epoch 116 ------------- Batch 600:\n",
      "Training Set Loss: 27.358864080928964 -------- Dev Set Loss: 1830.9546232010914\n",
      "Epoch 116 ------------- Batch 800:\n",
      "Training Set Loss: 31.96598584508315 -------- Dev Set Loss: 1815.9787379512848\n",
      "Epoch 117 ------------- Batch 0:\n",
      "Training Set Loss: 31.242943512989918 -------- Dev Set Loss: 1802.660415478172\n",
      "Epoch 117 ------------- Batch 200:\n",
      "Training Set Loss: 30.278590431175427 -------- Dev Set Loss: 1839.9873156835706\n",
      "Epoch 117 ------------- Batch 400:\n",
      "Training Set Loss: 36.2831402228393 -------- Dev Set Loss: 1842.1308427634456\n",
      "Epoch 117 ------------- Batch 600:\n",
      "Training Set Loss: 26.071525491858875 -------- Dev Set Loss: 1845.308744485314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117 ------------- Batch 800:\n",
      "Training Set Loss: 29.21181759013785 -------- Dev Set Loss: 1829.7668754327765\n",
      "Epoch 118 ------------- Batch 0:\n",
      "Training Set Loss: 29.44393935488476 -------- Dev Set Loss: 1819.3629339933652\n",
      "Epoch 118 ------------- Batch 200:\n",
      "Training Set Loss: 28.887140988927637 -------- Dev Set Loss: 1858.625513158493\n",
      "Epoch 118 ------------- Batch 400:\n",
      "Training Set Loss: 37.30673806426399 -------- Dev Set Loss: 1853.3274733649437\n",
      "Epoch 118 ------------- Batch 600:\n",
      "Training Set Loss: 26.006220432268194 -------- Dev Set Loss: 1855.348166020643\n",
      "Epoch 118 ------------- Batch 800:\n",
      "Training Set Loss: 27.823303778140872 -------- Dev Set Loss: 1839.3389037042757\n",
      "Epoch 119 ------------- Batch 0:\n",
      "Training Set Loss: 28.798016226899506 -------- Dev Set Loss: 1833.1058374286822\n",
      "Epoch 119 ------------- Batch 200:\n",
      "Training Set Loss: 26.937830757213334 -------- Dev Set Loss: 1877.0541153618083\n",
      "Epoch 119 ------------- Batch 400:\n",
      "Training Set Loss: 34.002227670599765 -------- Dev Set Loss: 1864.8827761690427\n",
      "Epoch 119 ------------- Batch 600:\n",
      "Training Set Loss: 24.499724378249 -------- Dev Set Loss: 1865.4201371888491\n",
      "Epoch 119 ------------- Batch 800:\n",
      "Training Set Loss: 25.565565346538968 -------- Dev Set Loss: 1845.3763398196188\n",
      "Epoch 120 ------------- Batch 0:\n",
      "Training Set Loss: 27.5779609565395 -------- Dev Set Loss: 1846.6354528853908\n",
      "Epoch 120 ------------- Batch 200:\n",
      "Training Set Loss: 22.978361381635096 -------- Dev Set Loss: 1882.428777594564\n",
      "Epoch 120 ------------- Batch 400:\n",
      "Training Set Loss: 32.24159198061027 -------- Dev Set Loss: 1873.984887529303\n",
      "Epoch 120 ------------- Batch 600:\n",
      "Training Set Loss: 23.5014094370675 -------- Dev Set Loss: 1873.4138061517456\n",
      "Epoch 120 ------------- Batch 800:\n",
      "Training Set Loss: 24.229923310881624 -------- Dev Set Loss: 1855.4002771152218\n",
      "Epoch 121 ------------- Batch 0:\n",
      "Training Set Loss: 26.67856374207898 -------- Dev Set Loss: 1858.9786728221588\n",
      "Epoch 121 ------------- Batch 200:\n",
      "Training Set Loss: 21.860570911737582 -------- Dev Set Loss: 1888.2952862138354\n",
      "Epoch 121 ------------- Batch 400:\n",
      "Training Set Loss: 31.54596579658367 -------- Dev Set Loss: 1885.361097508055\n",
      "Epoch 121 ------------- Batch 600:\n",
      "Training Set Loss: 22.98250177673983 -------- Dev Set Loss: 1880.475253218075\n",
      "Epoch 121 ------------- Batch 800:\n",
      "Training Set Loss: 22.88032724323704 -------- Dev Set Loss: 1864.5910810941537\n",
      "Epoch 122 ------------- Batch 0:\n",
      "Training Set Loss: 25.652752202476453 -------- Dev Set Loss: 1870.359476525258\n",
      "Epoch 122 ------------- Batch 200:\n",
      "Training Set Loss: 21.287015972898597 -------- Dev Set Loss: 1895.440036182565\n",
      "Epoch 122 ------------- Batch 400:\n",
      "Training Set Loss: 29.774795597261352 -------- Dev Set Loss: 1897.0480000463956\n",
      "Epoch 122 ------------- Batch 600:\n",
      "Training Set Loss: 23.079992744586328 -------- Dev Set Loss: 1881.194971468074\n",
      "Epoch 122 ------------- Batch 800:\n",
      "Training Set Loss: 22.252200996422125 -------- Dev Set Loss: 1876.9237763689266\n",
      "Epoch 123 ------------- Batch 0:\n",
      "Training Set Loss: 24.98408328322887 -------- Dev Set Loss: 1878.8522035236288\n",
      "Epoch 123 ------------- Batch 200:\n",
      "Training Set Loss: 20.937117867829727 -------- Dev Set Loss: 1910.3079939280474\n",
      "Epoch 123 ------------- Batch 400:\n",
      "Training Set Loss: 28.48045861210831 -------- Dev Set Loss: 1906.5046858020546\n",
      "Epoch 123 ------------- Batch 600:\n",
      "Training Set Loss: 22.922686895676147 -------- Dev Set Loss: 1890.0169886850554\n",
      "Epoch 123 ------------- Batch 800:\n",
      "Training Set Loss: 20.81775898262256 -------- Dev Set Loss: 1883.1085724867437\n",
      "Epoch 124 ------------- Batch 0:\n",
      "Training Set Loss: 24.525977005180728 -------- Dev Set Loss: 1887.8247746213817\n",
      "Epoch 124 ------------- Batch 200:\n",
      "Training Set Loss: 19.982365815829503 -------- Dev Set Loss: 1916.7954833992028\n",
      "Epoch 124 ------------- Batch 400:\n",
      "Training Set Loss: 27.278338150423252 -------- Dev Set Loss: 1916.6227890840485\n",
      "Epoch 124 ------------- Batch 600:\n",
      "Training Set Loss: 22.495857354813175 -------- Dev Set Loss: 1893.3868677271143\n",
      "Epoch 124 ------------- Batch 800:\n",
      "Training Set Loss: 19.53067413931396 -------- Dev Set Loss: 1886.9067460348897\n",
      "Epoch 125 ------------- Batch 0:\n",
      "Training Set Loss: 23.64826914698657 -------- Dev Set Loss: 1898.2255305972237\n",
      "Epoch 125 ------------- Batch 200:\n",
      "Training Set Loss: 19.170041989048674 -------- Dev Set Loss: 1917.131195233454\n",
      "Epoch 125 ------------- Batch 400:\n",
      "Training Set Loss: 26.822821173656457 -------- Dev Set Loss: 1922.3488324744185\n",
      "Epoch 125 ------------- Batch 600:\n",
      "Training Set Loss: 23.01167072924008 -------- Dev Set Loss: 1897.8784238654157\n",
      "Epoch 125 ------------- Batch 800:\n",
      "Training Set Loss: 18.715055643406444 -------- Dev Set Loss: 1896.1634830571293\n",
      "Epoch 126 ------------- Batch 0:\n",
      "Training Set Loss: 22.743921740424845 -------- Dev Set Loss: 1905.1518185539512\n",
      "Epoch 126 ------------- Batch 200:\n",
      "Training Set Loss: 18.442815941533926 -------- Dev Set Loss: 1920.1219102164337\n",
      "Epoch 126 ------------- Batch 400:\n",
      "Training Set Loss: 28.856526315855938 -------- Dev Set Loss: 1927.8115934568793\n",
      "Epoch 126 ------------- Batch 600:\n",
      "Training Set Loss: 21.93335483419237 -------- Dev Set Loss: 1892.2426944064616\n",
      "Epoch 126 ------------- Batch 800:\n",
      "Training Set Loss: 17.575690367780822 -------- Dev Set Loss: 1902.3918273118063\n",
      "Epoch 127 ------------- Batch 0:\n",
      "Training Set Loss: 22.7480041345665 -------- Dev Set Loss: 1913.6233420729666\n",
      "Epoch 127 ------------- Batch 200:\n",
      "Training Set Loss: 17.913818841833912 -------- Dev Set Loss: 1926.2427422441717\n",
      "Epoch 127 ------------- Batch 400:\n",
      "Training Set Loss: 31.610656156681422 -------- Dev Set Loss: 1932.870828652321\n",
      "Epoch 127 ------------- Batch 600:\n",
      "Training Set Loss: 19.301915436139517 -------- Dev Set Loss: 1898.7350465310342\n",
      "Epoch 127 ------------- Batch 800:\n",
      "Training Set Loss: 16.640195347517093 -------- Dev Set Loss: 1910.7979741681295\n",
      "Epoch 128 ------------- Batch 0:\n",
      "Training Set Loss: 21.82510782217348 -------- Dev Set Loss: 1922.048224202385\n",
      "Epoch 128 ------------- Batch 200:\n",
      "Training Set Loss: 17.379033877382845 -------- Dev Set Loss: 1940.2413489290377\n",
      "Epoch 128 ------------- Batch 400:\n",
      "Training Set Loss: 25.468381397794246 -------- Dev Set Loss: 1948.0339348677837\n",
      "Epoch 128 ------------- Batch 600:\n",
      "Training Set Loss: 19.149561076334148 -------- Dev Set Loss: 1909.7269587584994\n",
      "Epoch 128 ------------- Batch 800:\n",
      "Training Set Loss: 16.480666645703494 -------- Dev Set Loss: 1918.9912500799442\n",
      "Epoch 129 ------------- Batch 0:\n",
      "Training Set Loss: 21.456704179219596 -------- Dev Set Loss: 1931.8659599130028\n",
      "Epoch 129 ------------- Batch 200:\n",
      "Training Set Loss: 16.86841212031534 -------- Dev Set Loss: 1945.6764594251329\n",
      "Epoch 129 ------------- Batch 400:\n",
      "Training Set Loss: 28.89473838675517 -------- Dev Set Loss: 1950.7226946899457\n",
      "Epoch 129 ------------- Batch 600:\n",
      "Training Set Loss: 20.520683558374948 -------- Dev Set Loss: 1915.8668060248842\n",
      "Epoch 129 ------------- Batch 800:\n",
      "Training Set Loss: 16.088259821694717 -------- Dev Set Loss: 1929.7512014644751\n",
      "Epoch 130 ------------- Batch 0:\n",
      "Training Set Loss: 20.9904848289496 -------- Dev Set Loss: 1942.6792095416056\n",
      "Epoch 130 ------------- Batch 200:\n",
      "Training Set Loss: 16.67493877870539 -------- Dev Set Loss: 1951.908795404682\n",
      "Epoch 130 ------------- Batch 400:\n",
      "Training Set Loss: 28.648886111817223 -------- Dev Set Loss: 1958.4077087727712\n",
      "Epoch 130 ------------- Batch 600:\n",
      "Training Set Loss: 17.323024829927498 -------- Dev Set Loss: 1919.9670559200756\n",
      "Epoch 130 ------------- Batch 800:\n",
      "Training Set Loss: 15.665278872829228 -------- Dev Set Loss: 1936.9841127908098\n",
      "Epoch 131 ------------- Batch 0:\n",
      "Training Set Loss: 20.018839942419294 -------- Dev Set Loss: 1950.8600099915536\n",
      "Epoch 131 ------------- Batch 200:\n",
      "Training Set Loss: 16.249339598289012 -------- Dev Set Loss: 1963.2086929145166\n",
      "Epoch 131 ------------- Batch 400:\n",
      "Training Set Loss: 29.10273962740646 -------- Dev Set Loss: 1968.6054470641475\n",
      "Epoch 131 ------------- Batch 600:\n",
      "Training Set Loss: 17.123044325085854 -------- Dev Set Loss: 1929.368855145088\n",
      "Epoch 131 ------------- Batch 800:\n",
      "Training Set Loss: 15.076268357080997 -------- Dev Set Loss: 1946.8726982816104\n",
      "Epoch 132 ------------- Batch 0:\n",
      "Training Set Loss: 19.250498886104754 -------- Dev Set Loss: 1960.704491624653\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-bb03d903b69d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Back Propagation and Gradient Descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mfinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_batched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m256\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m256\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m256\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mfinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-49eda62325c6>\u001b[0m in \u001b[0;36mback_propagate\u001b[1;34m(self, y_pred, y_true)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mdz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrad_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-3718dbd4c93e>\u001b[0m in \u001b[0;36mback_prop\u001b[1;34m(self, dz)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdz\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-36c63361b492>\u001b[0m in \u001b[0;36mback_prop\u001b[1;34m(self, dy)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_dev = []\n",
    "loss_train = 0\n",
    "for epoch in range(num_of_epochs):\n",
    "    for batch in range(num_batches):\n",
    "        t = epoch * num_batches + batch + 1 # Step Number\n",
    "        \n",
    "         \n",
    "        if batch % 200 == 0:\n",
    "            y_dev_pred = final_model.forward_propagate(dev_x)\n",
    "            loss_dev = final_model.calculate_loss(y_dev_pred, dev_y)\n",
    "            losses_dev.append(loss_dev)\n",
    "            \n",
    "            print(f\"Epoch {epoch} ------------- Batch {batch}:\")\n",
    "            print(f\"Training Set Loss: {loss_train} -------- Dev Set Loss: {loss_dev}\")\n",
    "            \n",
    "        # Finding Y pred\n",
    "        y_pred = final_model.forward_propagate(train_x_batched[:, 0 + 256 * batch: 256 + 256 * batch])\n",
    "        \n",
    "        # Finding and Storing Loss\n",
    "        loss_train = final_model.calculate_loss(y_pred, train_y_batched[:, 0 + 256 * batch: 256 + 256 * batch])\n",
    "        losses_train.append(loss_train)\n",
    "       \n",
    "        \n",
    "        \n",
    "        # Back Propagation and Gradient Descent\n",
    "        final_model.back_propagate(y_pred, train_y_batched[:, 0 + 256 * batch: 256 + 256 * batch])\n",
    "        final_model.grad_descent(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de09ce66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(losses_dev[2:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9dc78f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train[1000:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc8838",
   "metadata": {},
   "source": [
    "# Now to measure the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad71b8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    result =  (y_true - 20 < y_pred) * (y_pred < y_true + 20)\n",
    "    return (np.sum(result)/result.size) * 100\n",
    "\n",
    "get_accuracy(np.array([32, 45, 62, 12]).T, np.array([33, 13414, 10, 30]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe457396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447, 9823)\n",
      "(1, 9823)\n"
     ]
    }
   ],
   "source": [
    "thres_overs = 10\n",
    "\n",
    "mask = (dev_data_x['Over'] > thres_overs).to_numpy()\n",
    "dev_x = dev_data_x.to_numpy().T\n",
    "dev_y = dev_data_y.to_numpy().T\n",
    "\n",
    "prediction_x = dev_x[:, mask]\n",
    "prediction_y = dev_y[:, mask]\n",
    "print(prediction_x.shape)\n",
    "print(prediction_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02c957ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1862.735022776114\n",
      "37.045709050188336\n"
     ]
    }
   ],
   "source": [
    "y_pred = final_model.forward_propagate(prediction_x)\n",
    "loss_pred = final_model.calculate_loss(y_pred, prediction_y)\n",
    "accuracy = get_accuracy(prediction_y, y_pred)\n",
    "print(loss_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1bd509ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[170.92242721, 169.18901964, 170.68081393, ..., 148.8274515 ,\n",
       "        148.91428421, 148.50415154]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bd0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
